{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedprox.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"fedprox.py\"\n",
    "# note to ming: apply_prox = true in main loop\n",
    "# note to ming: and add mu to the config code\n",
    "# in orignal paper, mu set is {0.001, 0.01, 0.1, 1}\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import metrics.writer as metrics_writer\n",
    "\n",
    "from baseline_constants import MAIN_PARAMS, MODEL_PARAMS\n",
    "from client import Client\n",
    "from server import Server\n",
    "from model import ServerModel\n",
    "from utils.constants import DATASETS\n",
    "\n",
    "\n",
    "STAT_METRICS_PATH = 'metrics/stat_metrics.csv'\n",
    "SYS_METRICS_PATH = 'metrics/sys_metrics.csv'\n",
    "\n",
    "def online(clients):\n",
    "    \"\"\"We assume all users are always online.\"\"\"\n",
    "    return clients\n",
    "\n",
    "def save_model(server_model, dataset, model):\n",
    "    \"\"\"Saves the given server model on checkpoints/dataset/model.ckpt.\"\"\"\n",
    "    # Save server model\n",
    "    ckpt_path = os.path.join('checkpoints', dataset)\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "    save_path = server_model.save(os.path.join(ckpt_path, '%s.ckpt' % model))\n",
    "    print('Model saved in path: %s' % save_path)\n",
    "\n",
    "\n",
    "def print_metrics(metrics, weights):\n",
    "    ordered_weights = [weights[c] for c in sorted(weights)]\n",
    "    metric_names = metrics_writer.get_metrics_names(metrics)\n",
    "    for metric in metric_names:\n",
    "        ordered_metric = [metrics[c][metric] for c in sorted(metrics)]\n",
    "        print('%s: %g, 10th percentile: %g, 90th percentile %g' \\\n",
    "              % (metric,\n",
    "                 np.average(ordered_metric, weights=ordered_weights),\n",
    "                 np.percentile(ordered_metric, 10),\n",
    "                 np.percentile(ordered_metric, 90)))\n",
    "    fom = [metrics[c][metric_names[0]] for c in sorted(metrics)]\n",
    "    final = np.average(fom, weights=ordered_weights)\n",
    "    return final\n",
    "\n",
    "class Fedprox_Trainer:\n",
    "    \n",
    "    def __init__(self, users, groups, train_data, test_data):\n",
    "        self.users = users\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "    def model_config(self, config, dataset, my_model):   \n",
    "        shared_model = my_model\n",
    "        model_path = '%s/%s.py' % (dataset, shared_model)\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please specify a valid dataset and a valid model.')\n",
    "        model_path = '%s.%s' % (dataset, shared_model)\n",
    "\n",
    "        print('############################## %s ##############################' % model_path)\n",
    "        mod = importlib.import_module(model_path)\n",
    "        ClientModel = getattr(mod, 'ClientProxModel')  \n",
    "        # Suppress tf warnings\n",
    "        tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "        # Create 2 models\n",
    "        model_params = MODEL_PARAMS[model_path]\n",
    "        model_params_list = list(model_params)\n",
    "        model_params_list[0] = config[\"lr\"]\n",
    "        model_params_list[1] = config[\"mu\"]\n",
    "        model_params = tuple(model_params_list)\n",
    "        tf.reset_default_graph()\n",
    "        client_model = ClientModel(config[\"seed\"], *model_params)\n",
    "\n",
    "        # Create server\n",
    "        server = Server(client_model)\n",
    "\n",
    "        # Create clients\n",
    "        _users = self.users\n",
    "        groups = [[] for _ in _users]\n",
    "        clients =  [Client(u, g, self.train_data[u], self.test_data[u], client_model) \\\n",
    "                    for u, g in zip(_users, groups)]\n",
    "        print('%d Clients in Total' % len(clients)) \n",
    "        return clients, server, client_model\n",
    "    \n",
    "    def begins(self, config, args):\n",
    "        clients, server, client_model = self.model_config(config, args.dataset, 'cnn_prox')  \n",
    "    \n",
    "        num_rounds = config[\"num-rounds\"]\n",
    "        eval_every = config[\"eval-every\"]\n",
    "        epochs_per_round = config['epochs']\n",
    "        batch_size = config['batch-size']\n",
    "        clients_per_round = config[\"clients-per-round\"]\n",
    "        \n",
    "        # Test untrained model on all clients\n",
    "        stat_metrics = server.test_model(clients)\n",
    "        all_ids, all_groups, all_num_samples = server.get_clients_info(clients)\n",
    "    #     metrics_writer.print_metrics(0, all_ids, stat_metrics, all_groups, all_num_samples, STAT_METRICS_PATH)\n",
    "        print_metrics(stat_metrics, all_num_samples)\n",
    "\n",
    "        # Simulate training\n",
    "        micro_acc = 0.\n",
    "        for i in range(num_rounds):\n",
    "            print('--- Round %d of %d: Training %d Clients ---' % (i+1, num_rounds, clients_per_round))\n",
    "\n",
    "            server.select_clients(online(clients), num_clients=clients_per_round)\n",
    "            c_ids, c_groups, c_num_samples = server.get_clients_info(None)\n",
    "\n",
    "            sys_metics = server.train_model(single_center=None, num_epochs=epochs_per_round, batch_size=batch_size, minibatch=None, apply_prox=True)\n",
    "            server.update_model_wmode()\n",
    "\n",
    "            # Test model on all clients\n",
    "            if (i + 1) % eval_every == 0 or (i + 1) == num_rounds:\n",
    "                stat_metrics = server.test_model(clients)\n",
    "                micro_acc = print_metrics(stat_metrics, all_num_samples)\n",
    "\n",
    "        # Save server model\n",
    "    #     save_model(server_model, args.dataset, shared_model)\n",
    "\n",
    "        # Close models\n",
    "    #     server_model.close()\n",
    "        client_model.close() \n",
    "        return micro_acc\n",
    "    \n",
    "    def ends(self):\n",
    "        print(\"experiment of Fedprox finished.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
