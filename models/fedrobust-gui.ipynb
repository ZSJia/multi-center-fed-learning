{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5411002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting alg_robust.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile alg_robust.py\n",
    "import numpy as np\n",
    "import random\n",
    "from math import modf, log\n",
    "from scipy.spatial.distance import cdist\n",
    "from kbmom.kmedianpp import euclidean_distances, kmedianpp_init\n",
    "\n",
    "class KbMOM:\n",
    "    \n",
    "    def __init__(self,X,K,nbr_blocks,coef_ech = 6,max_iter = 10,outliers = None, confidence = 0.95, threshold = 0.001,quantile   = 0.5,initial_centers = None,init_type ='km++',averaging_strategy='cumul'):\n",
    "        '''\n",
    "        # X             : numpy array = contains the data we want to cluster\n",
    "        # K             : number of clusters\n",
    "        # nbr_blocks    : number of blocks to create in init and loop\n",
    "        # coef_ech      : NUMBER of data in each block and cluster\n",
    "        # quantile      : quantile to keep for the empirical risk; by default the median\n",
    "        # max_iter      : number of iterations of the algorithm\n",
    "        # max_iter_init : number of iterations to run for the kmeans in the initilization procedure\n",
    "        # kmeanspp      : boolean. If true then init by kmeanspp else kmedianpp\n",
    "        # outliers      : number of supposed outliers\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # the structure of X_blocks and centers needs to to be modified.\n",
    "        # they need to store the whole parameters of a model.\n",
    "        # but when computing distance or predicting, just use the last\n",
    "        # layer of the parameters.\n",
    "        # to faciliate this computing, just added a function last_layer\n",
    "        \n",
    "        # Thus each item in X is a [n_layers] model, X is by number of clients array of numpy data.\n",
    "        '''\n",
    "        \n",
    "        # given element\n",
    "        self.X          = None\n",
    "        self.K          = K\n",
    "        self.max_iter   = max_iter\n",
    "        self.n, self.p  = len(X), X[0][24].shape[0] * X[0][24].shape[1]\n",
    "        self.quantile   = quantile\n",
    "        self.coef_ech   = coef_ech\n",
    "        self.B          = nbr_blocks\n",
    "        self.alpha      = 1 - confidence\n",
    "        self.threshold  = threshold\n",
    "        self.init_type  = init_type\n",
    "        self.averaging_strategy = averaging_strategy\n",
    "        self.n_layers = 24\n",
    "        \n",
    "        \n",
    "        # Test some given values\n",
    "        if outliers is not None:\n",
    "            self.outliers = outliers\n",
    "            t_sup = self.bloc_size(self.n,self.outliers)\n",
    "            if self.coef_ech > t_sup:\n",
    "                self.coef_ech  = max((t_sup-5),1)\n",
    "                self.coef_ech  = int(round(self.coef_ech))\n",
    "                print('warning:: the size of blocks has been computed according to the breakdown point theory')\n",
    "\n",
    "            B_sup = self.bloc_nb(self.n,self.outliers,b_size=self.coef_ech,alpha=self.alpha)\n",
    "            if self.B < B_sup :\n",
    "                self.B     = round(B_sup) + 10\n",
    "                self.B     = int(self.B)\n",
    "                print('warning:: the number of blocks has been computed according to the breakdown point theory')\n",
    "        \n",
    "        # Deal with exceptions:\n",
    "        if self.coef_ech <= self.K:\n",
    "            self.coef_ech = 2*self.K\n",
    "        \n",
    "        # internal element initialization\n",
    "        self.score         = np.ones((self.n,))\n",
    "        \n",
    "        if isinstance(initial_centers,np.ndarray):\n",
    "            self.centers = initial_centers\n",
    "        else:\n",
    "            self.centers = 0\n",
    "            \n",
    "        self.block_empirical_risk = []\n",
    "        self.median_block_centers = []\n",
    "        self.empirical_risk = []\n",
    "        self.iter           = 1\n",
    "        self.warnings       = 'None'\n",
    "    \n",
    "    def init_centers_function(self,X,idx_blocks):\n",
    "        '''\n",
    "        # Initialisation function: create nbr_blocks blocks, initialize with a kmeans++, \n",
    "        retrieve the index of the median block and its empirical risk value\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : numpy array of data\n",
    "        . idx_blocks : list of indices contained in the B blocks\n",
    "        '''\n",
    "        \n",
    "        # Blocks creation\n",
    "        size_of_blocks = self.coef_ech\n",
    "        \n",
    "        block_inertia = []\n",
    "        init_centers  = []\n",
    "        if self.init_type=='km++':\n",
    "            # instanciation of kmeans++\n",
    "            x_squared = X**2\n",
    "            x_squared_norms = x_squared.sum(axis=1)\n",
    "        \n",
    "            for idx_ in idx_blocks: \n",
    "                init_centers_ = kmedianpp_init(X[idx_,:], self.K, x_squared_norms[idx_], n_local_trials=None, square=True)\n",
    "                init_centers.append(init_centers_)\n",
    "                block_inertia.append(self.inertia_function(idx_,init_centers_))\n",
    "        else:\n",
    "            for idx_ in idx_blocks: \n",
    "                init_centers_ = self.random_init(X[idx_,:])\n",
    "                init_centers.append(init_centers_)\n",
    "                block_inertia.append(self.inertia_function(idx_,init_centers_))\n",
    "            \n",
    "        median_risk = sorted(block_inertia)[round(self.quantile*len(block_inertia))]\n",
    "\n",
    "        # Select the Q-quantile bloc\n",
    "        id_median = block_inertia.index(median_risk)\n",
    "        \n",
    "        # init centers\n",
    "        self.centers = init_centers[id_median]\n",
    "        \n",
    "        return(id_median,median_risk)\n",
    "    \n",
    "    def random_init(self,dataset):\n",
    "         return dataset[np.random.choice(len(dataset), self.K),:]\n",
    "        \n",
    "    def sampling_all_blocks_function(self):#,nbr_blocks,weighted_point,cluster_sizes):\n",
    "        '''\n",
    "        # Function which creates nbr_blocks blocks based on self.coef_ech and self.B\n",
    "        '''\n",
    "        blocks = [random.choices(np.arange(self.n),k = self.coef_ech) for i in range(self.B)]\n",
    "        return(blocks)\n",
    "    \n",
    "    \n",
    "    def inertia_function(self,idx_block,centroids = None):\n",
    "        '''\n",
    "        # Function which computes empirical risk per block\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : numpy array of data\n",
    "        . idx_block  : list of indices contained in the B blocks\n",
    "        . centroids  : if not None get the centers from kmeans++ initialisation\n",
    "        '''\n",
    "        if not isinstance(centroids,np.ndarray):\n",
    "            centroids = self.centers\n",
    "        \n",
    "        X_block           = [self.X[i] for i in idx_block]\n",
    "        nearest_centroid  = fed_dist(X_block,centroids,'sqeuclidean').argmin(axis=1)\n",
    "        \n",
    "        if len(set(nearest_centroid)) == self.K and sum(np.bincount(nearest_centroid) > 1) == self.K :\n",
    "            within_group_inertia = 0\n",
    "            for k,nc in enumerate(set(nearest_centroid)):\n",
    "                within_group_inertia += inertia_per_cluster(X_block, nearest_centroids, nc)\n",
    "            \n",
    "            return(within_group_inertia/len(idx_block))\n",
    "        else:\n",
    "            return(-1)\n",
    "     \n",
    "            \n",
    "    def median_risk_function(self,X,blocks):\n",
    "        '''\n",
    "        # Function which computes the sum of all within variances and return the index of the median block\n",
    "        and its empirical risk\n",
    "        \n",
    "        ```parameters ```       \n",
    "            . blocks     : list of indices forming the blocks\n",
    "            . X          : matrix of datapoints\n",
    "        '''\n",
    "        \n",
    "        block_inertia = list(map(self.inertia_function, blocks))\n",
    "            \n",
    "        nb_nonvalide_blocks = sum(np.array(block_inertia) == -1)\n",
    "        nb_valide_blocks    = int(self.B - nb_nonvalide_blocks)\n",
    "        \n",
    "        if nb_nonvalide_blocks != self.B:\n",
    "            \n",
    "            median_risk = sorted(block_inertia)[nb_nonvalide_blocks:][round(self.quantile*nb_valide_blocks)]\n",
    "            \n",
    "            # Select the Q-quantile bloc\n",
    "            id_median = block_inertia.index(median_risk)\n",
    "            return(id_median,median_risk)\n",
    "    \n",
    "        else:\n",
    "            return(None,None)\n",
    "        \n",
    "    def medianblock_centers_function(self,X,id_median,blocks):\n",
    "        '''\n",
    "        #compute the barycenter of each cluster in the median block\n",
    "        \n",
    "         ``` prms ```\n",
    "         . blocks     : list of indices forming the blocks\n",
    "         . X          : matrix of datapoints\n",
    "         . id_median  : index of the median block\n",
    "        '''\n",
    "        X_block           = [X[i] for i in blocks[id_median]]\n",
    "        distances         = fed_dist(X_block,self.centers,'sqeuclidean')\n",
    "        nearest_centroid  = distances.argmin(axis=1)\n",
    " \n",
    "        centers_ = [0] * len(set(nearest_centroid))\n",
    "        for k,nc in enumerate(set(nearest_centroid)):\n",
    "            block_clients = []\n",
    "            for i,v  in  enumerate( blocks[id_median] ) :\n",
    "                if nearest_centroid[i] == nc:\n",
    "                    c = Clients[i]\n",
    "                    block_clients.append(c)\n",
    "            _, upd = self.fed_func(self.centers[nc], block_clients)\n",
    "            centers[k] = server.update_model_nowmode()\n",
    "            cnt = 0\n",
    "            for i, v in enumerate( blocks[id_median] ):\n",
    "                if nearest_centroid[i] == nc:\n",
    "                    self.X[v] = upd[cnt]\n",
    "                    cnt += 1\n",
    "                             \n",
    "        self.centers = centers_\n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def weigthingscheme(self,median_block):\n",
    "        '''\n",
    "        Function which computes data depth\n",
    "        \n",
    "        ``` prms ```\n",
    "        . median_block: list containing the indices of data in the median block\n",
    "        \n",
    "        ''' \n",
    "        for idk in median_block:\n",
    "            self.score[idk] += 1\n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def fit(self,X, server, clients, fed_func):\n",
    "        '''\n",
    "        # Main loop of the K-bmom algorithm:\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : matrix of datapoints \n",
    "        '''\n",
    "        self.X = X\n",
    "        # initialisation step\n",
    "        if not isinstance(self.centers,np.ndarray):\n",
    "            idx_block = self.sampling_all_blocks_function()\n",
    "            id_median , median_risk_ = self.init_centers_function(X,idx_block)\n",
    "\n",
    "            # update\n",
    "            self.block_empirical_risk.append(median_risk_)\n",
    "            self.medianblock_centers_function(X,id_median,idx_block)\n",
    "            self.median_block_centers.append(self.centers)\n",
    "            self.empirical_risk.append(sum(fed_dist(X,self.centers,'sqeuclidean').min(axis=1))/self.n)\n",
    "            self.weigthingscheme(median_block=idx_block[id_median])\n",
    "        \n",
    "        if self.averaging_strategy == 'cumul':\n",
    "            cumul_centers_ = self.centers\n",
    "        \n",
    "        # Main Loop - fitting process\n",
    "        if (self.max_iter == 0):\n",
    "            condition = False\n",
    "        else:\n",
    "            condition = True\n",
    "       \n",
    "        while condition:\n",
    "\n",
    "            # sampling\n",
    "            idx_block = self.sampling_all_blocks_function()\n",
    "            \n",
    "            # Compute empirical risk for all blocks and select the empirical-block\n",
    "            id_median , median_risk_ = self.median_risk_function(X,idx_block)\n",
    "            \n",
    "            # If blocks are undefined, then restarting strategy\n",
    "            loop_within = 0\n",
    "            while (id_median == None) and loop_within < 10:\n",
    "                idx_block = self.sampling_all_blocks_function()\n",
    "                id_median , median_risk_ = self.init_centers_function(X,idx_block)\n",
    "                cumul_centers_  = np.zeros((self.K,self.p))\n",
    "                self.warnings = 'restart'\n",
    "                loop_within += 1\n",
    "            \n",
    "            if id_median == None:\n",
    "                self.iter = self.max_iter\n",
    "                self.warnings = 'algorithm did not converge'\n",
    "                condition = False\n",
    "                \n",
    "            else:\n",
    "                # update all parameters\n",
    "                self.block_empirical_risk.append(median_risk_)\n",
    "                self.medianblock_centers_function(X,id_median,idx_block, fed_func)\n",
    "                self.median_block_centers.append(self.centers)\n",
    "                self.empirical_risk.append(sum(cdist(X,self.centers,'sqeuclidean').min(axis=1))/self.n)\n",
    "                self.weigthingscheme(median_block=idx_block[id_median])\n",
    "\n",
    "                if self.averaging_strategy == 'cumul' and self.iter > (self.max_iter - 10):\n",
    "                    decay = self.max_iter - 10\n",
    "                    #current_centers = self.pivot(self.centers,cumul_centers_)\n",
    "                    cumul_centers_  = (self.centers / (self.iter - decay)) + (self.iter-decay - 1)/(self.iter - decay) * cumul_centers_\n",
    "                    self.centers = cumul_centers_\n",
    "\n",
    "                self.iter += 1\n",
    "                if self.iter>=self.max_iter:\n",
    "                    condition = False\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Function which computes the partition based on the centroids of Median Block \n",
    "        '''\n",
    "        D_nk = fed_dist(X,self.centers,'sqeuclidean')\n",
    "        return(D_nk.argmin(axis=1))\n",
    "    \n",
    "\n",
    "    def bloc_size(self,n_sample,n_outliers):\n",
    "        '''\n",
    "        Function which fits the maximum size of blocks before a the breakpoint\n",
    "        ```prms```\n",
    "        n_sample: nb of data\n",
    "        n_outlier: nb of outliers\n",
    "        '''\n",
    "        return(log(2.)/log(1/(1- (n_outliers/n_sample))))\n",
    "\n",
    "\n",
    "    def bloc_nb(self,n_sample,n_outliers,b_size=None,alpha=0.05):\n",
    "        '''\n",
    "        Function which fits the minimum nb of blocks for a given size t before a the breakpoint\n",
    "        ```prms```\n",
    "        n_sample: nb of data\n",
    "        n_outlier: nb of outliers\n",
    "        b_size = bloc_size\n",
    "        alpha : threshold confiance\n",
    "        '''\n",
    "        if n_outliers/n_sample >= 0.5:\n",
    "            print('too much noise')\n",
    "            return()\n",
    "        elif b_size == None:\n",
    "            t = bloc_size(n_sample,n_outliers)\n",
    "            return(log(1/alpha) / (2* ((1-n_outliers/n_sample)**t - 1/2)**2))\n",
    "        else:\n",
    "            t = b_size\n",
    "            return(log(1/alpha) / (2* ((1-n_outliers/n_sample)**t - 1/2)**2))\n",
    "   \n",
    "    def stopping_crit(self,risk_median):\n",
    "        risk_ = risk_median[::-1][:3]\n",
    "        den = (risk_[2]-risk_[1])-(risk_[1]-risk_[0])\n",
    "        Ax = risk_[2] - (risk_[2]-risk_[1])**2/den\n",
    "        return(Ax)\n",
    "    \n",
    "    def stopping_crit_GMM(self,risk_median):\n",
    "        risk_ = risk_median[::-1][:3]\n",
    "        Aq   = (risk_[0] - risk_[1])/(risk_[1] - risk_[2])\n",
    "        \n",
    "        Rinf = risk_[1] + 1/(1-Aq)*(risk_[0] - risk_[1])\n",
    "        return(Rinf)\n",
    "        \n",
    "    def pivot(self,mu1,mu2):\n",
    "        error    = cdist(mu1,mu2).argmin(axis=1)\n",
    "        pivot_mu = np.zeros((self.K,self.p))\n",
    "        for i,j in enumerate(error):\n",
    "            pivot_mu[i,:] = mu1[j,:]\n",
    "        return(pivot_mu)\n",
    "    \n",
    "    def set_fed_func(self, func):\n",
    "        self.fed_func  = func\n",
    "        \n",
    "    def last_layer(self, X_list):\n",
    "        return [x[self.n_layers] for x in X_list]\n",
    "    \n",
    "    def fed_dist(Xa, Xb, method = 'sqeuclidean')\n",
    "        tran_xa = last_layer(Xa)\n",
    "        tran_xb = last_layer(Xb)\n",
    "        \n",
    "        xa = list()\n",
    "        xb = list()\n",
    "        for x in tran_xa:\n",
    "            xa.append(x.flatten())\n",
    "        for x in tran_xb:\n",
    "            xb.append(x.flatten())\n",
    "            \n",
    "        return cdist(np.array(xa), np.array(xb), method)\n",
    "    \n",
    "    def inertia_per_cluster(X_block, nearest_centroids, nc):\n",
    "        clus = list()\n",
    "        tran_x_block = last_layer(X_block)\n",
    "        for i, xb in enumerate(tran_x_block):\n",
    "            if nearest_centroids[i] == nc:\n",
    "                clus.append(xb.flatten())\n",
    "                \n",
    "        centers_ = np.array(clus).mean(axis = 0).reshape(1, -1)\n",
    "        return cdist(np.array(clus), centers_,'sqeuclidean').sum()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3791802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedrobust.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fedrobust.py\n",
    "# note to audience, fedrobust is based on the work of Median-of-means K-means,\n",
    "# author is Camille Saumard, his email is camille.brunet@gmail.com\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from math import modf, log\n",
    "from scipy.spatial.distance import cdist\n",
    "from kbmom.kmedianpp import euclidean_distances, kmedianpp_init\n",
    "\n",
    "# tensorflow is required for our experiment, please install tf 1.5 not 2\n",
    "import tensorflow as tf\n",
    "import metrics.writer as metrics_writer\n",
    "\n",
    "from baseline_constants import MAIN_PARAMS, MODEL_PARAMS\n",
    "from client import Client\n",
    "from server import Server\n",
    "from model import ServerModel\n",
    "from utils.constants import DATASETS\n",
    "from alg_robust import KbMOM\n",
    "\n",
    "\n",
    "STAT_METRICS_PATH = 'metrics/stat_metrics.csv'\n",
    "SYS_METRICS_PATH = 'metrics/sys_metrics.csv'\n",
    "\n",
    "def online(clients):\n",
    "    \"\"\"We assume all users are always online.\"\"\"\n",
    "    return clients\n",
    "\n",
    "def save_model(server_model, dataset, model):\n",
    "    \"\"\"Saves the given server model on checkpoints/dataset/model.ckpt.\"\"\"\n",
    "    # Save server model\n",
    "    ckpt_path = os.path.join('checkpoints', dataset)\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "    save_path = server_model.save(os.path.join(ckpt_path, '%s.ckpt' % model))\n",
    "    print('Model saved in path: %s' % save_path)\n",
    "\n",
    "\n",
    "def print_metrics(metrics, weights):\n",
    "    ordered_weights = [weights[c] for c in sorted(weights)]\n",
    "    metric_names = metrics_writer.get_metrics_names(metrics)\n",
    "    for metric in metric_names:\n",
    "        ordered_metric = [metrics[c][metric] for c in sorted(metrics)]\n",
    "        print('%s: %g, 10th percentile: %g, 90th percentile %g' \\\n",
    "              % (metric,\n",
    "                 np.average(ordered_metric, weights=ordered_weights),\n",
    "                 np.percentile(ordered_metric, 10),\n",
    "                 np.percentile(ordered_metric, 90)))\n",
    "    fom = [metrics[c][metric_names[0]] for c in sorted(metrics)]\n",
    "    final = np.average(fom, weights=ordered_weights)\n",
    "    return final\n",
    "\n",
    "class Fedrobust_Trainer:\n",
    "    \n",
    "    def __init__(self, users, groups, train_data, test_data):\n",
    "        self.users = users\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.num_clients_per_round = 0\n",
    "        \n",
    "    def model_config(self, config, dataset, my_model):   \n",
    "        shared_model = my_model\n",
    "        model_path = '%s/%s.py' % (dataset, shared_model)\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please specify a valid dataset and a valid model.')\n",
    "        model_path = '%s.%s' % (dataset, shared_model)\n",
    "\n",
    "        print('############################## %s ##############################' % model_path)\n",
    "        mod = importlib.import_module(model_path)\n",
    "        ClientModel = getattr(mod, 'ClientModel')  \n",
    "        # Suppress tf warnings\n",
    "        tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "        # Create 2 models\n",
    "        model_params = MODEL_PARAMS[model_path]\n",
    "        model_params_list = list(model_params)\n",
    "        model_params_list[0] = config[\"lr\"]\n",
    "        model_params = tuple(model_params_list)\n",
    "        tf.reset_default_graph()\n",
    "        client_model = ClientModel(config[\"seed\"], *model_params)\n",
    "\n",
    "        # Create server\n",
    "        server = Server(client_model)\n",
    "\n",
    "        # Create clients\n",
    "        _users = self.users\n",
    "        groups = [[] for _ in _users]\n",
    "        clients =  [Client(u, g, self.train_data[u], self.test_data[u], client_model) \\\n",
    "                    for u, g in zip(_users, groups)]\n",
    "        print('%d Clients in Total' % len(clients)) \n",
    "        return clients, server, client_model\n",
    "    \n",
    "    def fed_train(server, possible_clients, ite, config):\n",
    "        server.select_clients(possible_clients, num_clients=len(possible_clients))\n",
    "        c_ids, c_groups, c_num_samples = server.get_clients_info(None)\n",
    "        sys_metics = server.train_model(single_center=None, num_epochs=epochs_per_round, batch_size=batch_size, minibatch=None, apply_prox=True)\n",
    "        return updates\n",
    "         \n",
    "        return new_center\n",
    "    \n",
    "    def update_cluster(self):\n",
    "        return(self)\n",
    "    \n",
    "    def begins(self, config, args):\n",
    "        \n",
    "        def shout(text):\n",
    "            return text.upper()\n",
    "        \n",
    "        clients, server, client_model = self.model_config(config, args.dataset, 'cnn')  \n",
    "        \n",
    "        K = config[\"num-clusters\"]\n",
    "        num_rounds = config[\"num-rounds\"]\n",
    "        eval_every = config[\"eval-every\"]\n",
    "        epochs_per_round = config['epochs']\n",
    "        batch_size = config['batch-size']\n",
    "        clients_per_round = config[\"clients-per-round\"]\n",
    "        \n",
    "        all_ids, all_groups, all_num_samples = server.get_clients_info(clients)\n",
    "        # train all clients one round \n",
    "        _, tmp_data = server.train_model(None, 1, batch_size, None, clients, False)\n",
    "        all_cl_models = [x[1] for x in tmp_data ]\n",
    "        #print(\"the shape of a model is: \", len(all_cl_models[0][1]))\n",
    "        print(\"last layer of a model is:\", all_cl_models[0][24].shape)\n",
    "        \n",
    "        centers = [[1, 1], [-1, -1], [1, -1]]      \n",
    "        # self.all_cl_models, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n",
    "        robust_helper = KbMOM(X = all_cl_models, K = K, nbr_blocks = 40, coef_ech = int(len(clients) * 0.25) , quantile=0.5)\n",
    "              \n",
    "        #clients, server, client_model = self.model_config(config, args.dataset, 'cnn_prox')  \n",
    "\n",
    "        return 0.5\n",
    "    \n",
    "    def ends(self):\n",
    "        print(\"experiment of Fed Robust is finished.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec570972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
