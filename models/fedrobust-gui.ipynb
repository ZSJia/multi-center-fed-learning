{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6732d2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e968ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting robust_main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile robust_main.py\n",
    "import numpy as np\n",
    "import random\n",
    "from math import modf, log\n",
    "from scipy.spatial.distance import cdist\n",
    "from kbmom.kmedianpp import euclidean_distances, kmedianpp_init\n",
    "from kbmom.utils import loglikelihood, BIC\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "class KbMOM:\n",
    "    \n",
    "    def __init__(self,X,K,nbr_blocks,coef_ech = 6,max_iter = 40,outliers = None, confidence = 0.95, threshold = 0.001,quantile   = 0.5,initial_centers = None,init_type ='km++',averaging_strategy='cumul', n_layers = 1):\n",
    "        '''\n",
    "        # X             : numpy array = contains the data we want to cluster\n",
    "        # K             : number of clusters\n",
    "        # nbr_blocks    : number of blocks to create in init and loop\n",
    "        # coef_ech      : NUMBER of data in each block and cluster\n",
    "        # quantile      : quantile to keep for the empirical risk; by default the median\n",
    "        # max_iter      : number of iterations of the algorithm\n",
    "        # max_iter_init : number of iterations to run for the kmeans in the initilization procedure\n",
    "        # kmeanspp      : boolean. If true then init by kmeanspp else kmedianpp\n",
    "        # outliers      : number of supposed outliers\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # the structure of X_blocks and centers needs to to be modified.\n",
    "        # they need to store the whole parameters of a model.\n",
    "        # but when computing distance or predicting, just use the last\n",
    "        # layer of the parameters.\n",
    "        # to faciliate this computing, just added a function last_layer\n",
    "        \n",
    "        # Thus each item in X is a [n_layers] model, X is by number of clients array of numpy data.\n",
    "        '''\n",
    "        \n",
    "        # given element\n",
    "        self.X          = None\n",
    "        self.K          = K\n",
    "        self.max_iter   = max_iter\n",
    "        self.n, self.p  = len(X), X[0][n_layers].shape[0] * X[0][n_layers].shape[1]\n",
    "        self.quantile   = quantile\n",
    "        self.coef_ech   = coef_ech\n",
    "        self.B          = nbr_blocks\n",
    "        self.alpha      = 1 - confidence\n",
    "        self.threshold  = threshold\n",
    "        self.init_type  = init_type\n",
    "        self.averaging_strategy = averaging_strategy\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        \n",
    "        # Test some given values\n",
    "        if outliers is not None:\n",
    "            self.outliers = outliers\n",
    "            t_sup = self.bloc_size(self.n,self.outliers)\n",
    "            if self.coef_ech > t_sup:\n",
    "                self.coef_ech  = max((t_sup-5),1)\n",
    "                self.coef_ech  = int(round(self.coef_ech))\n",
    "                print('warning:: the size of blocks has been computed according to the breakdown point theory')\n",
    "\n",
    "            B_sup = self.bloc_nb(self.n,self.outliers,b_size=self.coef_ech,alpha=self.alpha)\n",
    "            if self.B < B_sup :\n",
    "                self.B     = round(B_sup) + 10\n",
    "                self.B     = int(self.B)\n",
    "                print('warning:: the number of blocks has been computed according to the breakdown point theory')\n",
    "        \n",
    "        # Deal with exceptions:\n",
    "        if self.coef_ech <= self.K:\n",
    "            self.coef_ech = 2*self.K\n",
    "        \n",
    "        # internal element initialization\n",
    "        self.score         = np.ones((self.n,))\n",
    "        \n",
    "        if isinstance(initial_centers,np.ndarray):\n",
    "            self.centers = initial_centers\n",
    "        else:\n",
    "            self.centers = 0\n",
    "            \n",
    "        self.block_empirical_risk = []\n",
    "        self.median_block_centers = []\n",
    "        self.empirical_risk = []\n",
    "        self.iter           = 1\n",
    "        self.warnings       = 'None'\n",
    "    \n",
    "    def init_centers_function(self,X,idx_blocks):\n",
    "        '''\n",
    "        # Initialisation function: create nbr_blocks blocks, initialize with a kmeans++, \n",
    "        retrieve the index of the median block and its empirical risk value\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : numpy array of data\n",
    "        . idx_blocks : list of indices contained in the B blocks\n",
    "        '''\n",
    "        \n",
    "        # Blocks creation\n",
    "        size_of_blocks = self.coef_ech\n",
    "        \n",
    "        block_inertia = []\n",
    "        init_centers  = []\n",
    "        if self.init_type=='km++':\n",
    "            # instanciation of kmeans++\n",
    "            x_squared = X**2\n",
    "            x_squared_norms = x_squared.sum(axis=1)\n",
    "        \n",
    "            for idx_ in idx_blocks: \n",
    "                init_centers_ = kmedianpp_init(X[idx_,:], self.K, x_squared_norms[idx_], n_local_trials=None, square=True)\n",
    "                init_centers.append(init_centers_)\n",
    "                block_inertia.append(self.inertia_function(idx_,init_centers_))\n",
    "        else:\n",
    "            for idx_ in idx_blocks: \n",
    "                init_centers_ = self.random_init([X[i] for i in idx_])\n",
    "                init_centers.append(init_centers_)\n",
    "                block_inertia.append(self.inertia_function(idx_,init_centers_))\n",
    "            \n",
    "        median_risk = sorted(block_inertia)[round(self.quantile*len(block_inertia))]\n",
    "\n",
    "        # Select the Q-quantile bloc\n",
    "        id_median = block_inertia.index(median_risk)\n",
    "        \n",
    "        # init centers\n",
    "        self.centers = init_centers[id_median]\n",
    "        \n",
    "        return(id_median,median_risk)\n",
    "    \n",
    "    def random_init(self,dataset):\n",
    "        rnd_ =  np.random.choice(len(dataset), self.K)\n",
    "        s = [dataset[i] for i in rnd_]\n",
    "        return s\n",
    "        \n",
    "    def sampling_all_blocks_function(self):#,nbr_blocks,weighted_point,cluster_sizes):\n",
    "        '''\n",
    "        # Function which creates nbr_blocks blocks based on self.coef_ech and self.B\n",
    "        '''\n",
    "        blocks = [random.choices(np.arange(self.n),k = self.coef_ech) for i in range(self.B)]\n",
    "        return(blocks)\n",
    "    \n",
    "    \n",
    "    def inertia_function(self,idx_block,centroids = None):\n",
    "        '''\n",
    "        # Function which computes empirical risk per block\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : numpy array of data\n",
    "        . idx_block  : list of indices contained in the B blocks\n",
    "        . centroids  : if not None get the centers from kmeans++ initialisation\n",
    "        '''\n",
    "        if not isinstance(centroids,list):\n",
    "            centroids = self.centers\n",
    "        \n",
    "#         print(\"The block contains:[\",  idx_block ,\"]\")\n",
    "        X_block           = [self.X[i] for i in idx_block]\n",
    "        nearest_centroid  = self.fed_dist(X_block,centroids,'sqeuclidean').argmin(axis=1)\n",
    "        \n",
    "        if len(set(nearest_centroid)) == self.K and sum(np.bincount(nearest_centroid) > 1) == self.K :\n",
    "            within_group_inertia = 0\n",
    "            for k,nc in enumerate(set(nearest_centroid)):\n",
    "                within_group_inertia += self.inertia_per_cluster(X_block, nearest_centroid, nc)\n",
    "            \n",
    "            return(within_group_inertia/len(idx_block))\n",
    "        else:\n",
    "            return(-1)\n",
    "     \n",
    "            \n",
    "    def median_risk_function(self,X,blocks):\n",
    "        '''\n",
    "        # Function which computes the sum of all within variances and return the index of the median block\n",
    "        and its empirical risk\n",
    "        \n",
    "        ```parameters ```       \n",
    "            . blocks     : list of indices forming the blocks\n",
    "            . X          : matrix of datapoints\n",
    "        '''\n",
    "        \n",
    "        block_inertia = list(map(self.inertia_function, blocks))\n",
    "            \n",
    "        nb_nonvalide_blocks = sum(np.array(block_inertia) == -1)\n",
    "        nb_valide_blocks    = int(self.B - nb_nonvalide_blocks)\n",
    "        \n",
    "        if nb_nonvalide_blocks != self.B:\n",
    "            \n",
    "            median_risk = sorted(block_inertia)[nb_nonvalide_blocks:][round(self.quantile*nb_valide_blocks)]\n",
    "            \n",
    "            # Select the Q-quantile bloc\n",
    "            id_median = block_inertia.index(median_risk)\n",
    "            return(id_median,median_risk)\n",
    "    \n",
    "        else:\n",
    "            return(None,None)\n",
    "        \n",
    "    def medianblock_centers_function(self,X,id_median,blocks):\n",
    "        '''\n",
    "        #compute the barycenter of each cluster in the median block\n",
    "        \n",
    "         ``` prms ```\n",
    "         . blocks     : list of indices forming the blocks\n",
    "         . X          : matrix of datapoints\n",
    "         . id_median  : index of the median block\n",
    "        '''\n",
    "        X_block           = [X[i] for i in blocks[id_median]]\n",
    "        distances         = self.fed_dist(X_block,self.centers,'sqeuclidean')\n",
    "        nearest_centroid  = distances.argmin(axis=1)\n",
    " \n",
    "        print(\"len of nearest centroid: \", len(set(nearest_centroid)))\n",
    "        centers_ = [0] * len(set(nearest_centroid))\n",
    "        for k,nc in enumerate(set(nearest_centroid)):\n",
    "            cl_block = []\n",
    "            for i,v  in  enumerate( blocks[id_median] ) :\n",
    "                if nearest_centroid[i] == nc:\n",
    "                    cl_block.append(v)\n",
    "            _, upd = self.E_func(self.centers[nc], cl_block)\n",
    "            centers_[k] = self.M_func()\n",
    "            cnt = 0\n",
    "            for i, v in enumerate( blocks[id_median] ):\n",
    "                if nearest_centroid[i] == nc:\n",
    "                    self.X[v] = upd[cnt][1] # update is a tuple which return by E_func, the 0 is a number of samples, the 1 is model\n",
    "                    cnt += 1\n",
    "                             \n",
    "        self.centers = centers_\n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def weigthingscheme(self,median_block):\n",
    "        '''\n",
    "        Function which computes data depth\n",
    "        \n",
    "        ``` prms ```\n",
    "        . median_block: list containing the indices of data in the median block\n",
    "        \n",
    "        ''' \n",
    "        for idk in median_block:\n",
    "            self.score[idk] += 1\n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def fit(self,X):\n",
    "        '''\n",
    "        # Main loop of the K-bmom algorithm:\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : matrix of datapoints \n",
    "        '''\n",
    "        self.X = X\n",
    "        # initialisation step\n",
    "        if not isinstance(self.centers,np.ndarray):\n",
    "            idx_block = self.sampling_all_blocks_function()\n",
    "            id_median , median_risk_ = self.init_centers_function(X,idx_block)\n",
    "\n",
    "            self.block_empirical_risk.append(median_risk_)\n",
    "            self.medianblock_centers_function(X, id_median,idx_block)\n",
    "            self.median_block_centers.append(self.centers)\n",
    "            self.empirical_risk.append(sum(self.fed_dist(self.X, self.centers,'sqeuclidean').min(axis=1))/self.n)\n",
    "            self.weigthingscheme(median_block=idx_block[id_median])\n",
    "        \n",
    "        if self.averaging_strategy == 'cumul':\n",
    "            cumul_centers_ = self.centers\n",
    "        \n",
    "        # Main Loop - fitting process\n",
    "        if (self.max_iter == 0):\n",
    "            condition = False\n",
    "        else:\n",
    "            condition = True\n",
    "       \n",
    "        while condition:\n",
    "            print('--- Round %d of %d: Training %d Clients ---' % (self.iter+1, self.max_iter, self.coef_ech))\n",
    "            # sampling\n",
    "            idx_block = self.sampling_all_blocks_function()\n",
    "            \n",
    "            # Compute empirical risk for all blocks and select the empirical-block\n",
    "            id_median , median_risk_ = self.median_risk_function(self.X,idx_block)\n",
    "            \n",
    "            # If blocks are undefined, then restarting strategy\n",
    "            loop_within = 0\n",
    "            while (id_median == None) and loop_within < 10:\n",
    "                idx_block = self.sampling_all_blocks_function()\n",
    "                id_median , median_risk_ = self.init_centers_function(self.X,idx_block)\n",
    "                cumul_centers_  = np.zeros((self.K,self.p))\n",
    "                self.warnings = 'restart'\n",
    "                loop_within += 1\n",
    "            \n",
    "            if id_median == None:\n",
    "                self.iter = self.max_iter\n",
    "                self.warnings = 'algorithm did not converge'\n",
    "                condition = False\n",
    "                \n",
    "            else:\n",
    "                # update all parameters\n",
    "                self.block_empirical_risk.append(median_risk_)\n",
    "                self.medianblock_centers_function(self.X,id_median,idx_block)\n",
    "                self.median_block_centers.append(self.centers)\n",
    "                self.empirical_risk.append(sum(self.fed_dist(self.X,self.centers,'sqeuclidean').min(axis=1))/self.n)\n",
    "                self.weigthingscheme(median_block=idx_block[id_median])\n",
    "\n",
    "#                 if self.averaging_strategy == 'cumul' and self.iter > (self.max_iter - 10):\n",
    "#                     decay = self.max_iter - 10\n",
    "#                     #current_centers = self.pivot(self.centers,cumul_centers_)\n",
    "#                     cumul_centers_  = (self.centers / (self.iter - decay)) + (self.iter-decay - 1)/(self.iter - decay) * cumul_centers_\n",
    "#                     self.centers = cumul_centers_\n",
    "\n",
    "                self.iter += 1\n",
    "                if self.iter>=self.max_iter:\n",
    "                    condition = False\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Function which computes the partition based on the centroids of Median Block \n",
    "        '''\n",
    "        D_nk = self.fed_dist(X,self.centers,'sqeuclidean')\n",
    "        return(D_nk.argmin(axis=1))\n",
    "    \n",
    "\n",
    "    def bloc_size(self,n_sample,n_outliers):\n",
    "        '''\n",
    "        Function which fits the maximum size of blocks before a the breakpoint\n",
    "        ```prms```\n",
    "        n_sample: nb of data\n",
    "        n_outlier: nb of outliers\n",
    "        '''\n",
    "        return(log(2.)/log(1/(1- (n_outliers/n_sample))))\n",
    "\n",
    "\n",
    "    def bloc_nb(self,n_sample,n_outliers,b_size=None,alpha=0.05):\n",
    "        '''\n",
    "        Function which fits the minimum nb of blocks for a given size t before a the breakpoint\n",
    "        ```prms```\n",
    "        n_sample: nb of data\n",
    "        n_outlier: nb of outliers\n",
    "        b_size = bloc_size\n",
    "        alpha : threshold confiance\n",
    "        '''\n",
    "        if n_outliers/n_sample >= 0.5:\n",
    "            print('too much noise')\n",
    "            return()\n",
    "        elif b_size == None:\n",
    "            t = bloc_size(n_sample,n_outliers)\n",
    "            return(log(1/alpha) / (2* ((1-n_outliers/n_sample)**t - 1/2)**2))\n",
    "        else:\n",
    "            t = b_size\n",
    "            return(log(1/alpha) / (2* ((1-n_outliers/n_sample)**t - 1/2)**2))\n",
    "   \n",
    "    def stopping_crit(self,risk_median):\n",
    "        risk_ = risk_median[::-1][:3]\n",
    "        den = (risk_[2]-risk_[1])-(risk_[1]-risk_[0])\n",
    "        Ax = risk_[2] - (risk_[2]-risk_[1])**2/den\n",
    "        return(Ax)\n",
    "    \n",
    "    def stopping_crit_GMM(self,risk_median):\n",
    "        risk_ = risk_median[::-1][:3]\n",
    "        Aq   = (risk_[0] - risk_[1])/(risk_[1] - risk_[2])\n",
    "        \n",
    "        Rinf = risk_[1] + 1/(1-Aq)*(risk_[0] - risk_[1])\n",
    "        return(Rinf)\n",
    "        \n",
    "    def pivot(self,mu1,mu2):\n",
    "        error    = cdist(mu1,mu2).argmin(axis=1)\n",
    "        pivot_mu = np.zeros((self.K,self.p))\n",
    "        for i,j in enumerate(error):\n",
    "            pivot_mu[i,:] = mu1[j,:]\n",
    "        return(pivot_mu)\n",
    "    \n",
    "    def set_E_func(self, func):\n",
    "        self.E_func  = func\n",
    "        \n",
    "    def set_M_func(self, func):\n",
    "        self.M_func = func\n",
    "        \n",
    "    def last_layer(self, X_list):\n",
    "        return [x[self.n_layers] for x in X_list]\n",
    "    \n",
    "    def fed_dist(self, Xa, Xb, method = 'sqeuclidean'):\n",
    "        xa_transformed = self.last_layer(Xa)\n",
    "        xb_transformed = self.last_layer(Xb)\n",
    "        \n",
    "        xa = list(map(lambda x: x.flatten(), xa_transformed))\n",
    "        xb = list(map(lambda x: x.flatten(), xb_transformed))\n",
    "            \n",
    "        return cdist(np.array(xa), np.array(xb), method)\n",
    "    \n",
    "    def inertia_per_cluster(self, X_block, nearest_centroids, nc):\n",
    "        clster = list()\n",
    "        tran_x_block = self.last_layer(X_block)\n",
    "        for i, xb in enumerate(tran_x_block):\n",
    "            if nearest_centroids[i] == nc:\n",
    "                clster.append(xb.flatten())\n",
    "                \n",
    "        centers_ = np.array(clster).mean(axis = 0).reshape(1, -1)\n",
    "        return cdist(np.array(clster), centers_,'sqeuclidean').sum()\n",
    "    \n",
    "    def npx(self):\n",
    "        x_transformed = self.last_layer(self.X)\n",
    "        \n",
    "        x = list(map(lambda x: x.flatten(), x_transformed))\n",
    "        return np.array(x)\n",
    "    \n",
    "    def npcenters(self):\n",
    "        x_tran = self.last_layer(self.centers)\n",
    "        x = list(map(lambda x: x.flatten(), x_tran))\n",
    "        return np.array(x)\n",
    "    \n",
    "    def loglik(self):\n",
    "        return loglikelihood(self.npx(), self.npcenters())\n",
    "\n",
    "    def BIC(self):\n",
    "        return BIC(self.npx(), self.npcenters())\n",
    "    \n",
    "    def sil_score(self):\n",
    "        lbl = self.predict(self.X)\n",
    "        return silhouette_score(self.npx(), lbl, metric='euclidean')\n",
    "\n",
    "    def DB_score(self):\n",
    "        lbl = self.predict(self.X)\n",
    "        return davies_bouldin_score(self.npx(), lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39035ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedrobust.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fedrobust.py\n",
    "# note to audience, fedrobust is based on the work of Median-of-means K-means,\n",
    "# author is Camille Saumard, his email is camille.brunet@gmail.com\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from math import modf, log\n",
    "from scipy.spatial.distance import cdist\n",
    "from kbmom.kmedianpp import euclidean_distances, kmedianpp_init\n",
    "\n",
    "# tensorflow is required for our experiment, please install tf 1.5 not 2\n",
    "import tensorflow as tf\n",
    "import metrics.writer as metrics_writer\n",
    "\n",
    "from baseline_constants import MAIN_PARAMS, MODEL_PARAMS\n",
    "from client import Client\n",
    "from server import Server, MDLpoisonServer\n",
    "from model import ServerModel\n",
    "from utils.constants import DATASETS\n",
    "from robust_main import KbMOM\n",
    "\n",
    "from mlhead_utilfuncs import log_history, save_historyfile\n",
    "\n",
    "\n",
    "STAT_METRICS_PATH = 'metrics/stat_metrics.csv'\n",
    "SYS_METRICS_PATH = 'metrics/sys_metrics.csv'\n",
    "\n",
    "def online(clients):\n",
    "    \"\"\"We assume all users are always online.\"\"\"\n",
    "    return clients\n",
    "\n",
    "def save_model(server_model, dataset, model):\n",
    "    \"\"\"Saves the given server model on checkpoints/dataset/model.ckpt.\"\"\"\n",
    "    # Save server model\n",
    "    ckpt_path = os.path.join('checkpoints', dataset)\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "    save_path = server_model.save(os.path.join(ckpt_path, '%s.ckpt' % model))\n",
    "    print('Model saved in path: %s' % save_path)\n",
    "\n",
    "\n",
    "def print_metrics(metrics, weights):\n",
    "    ordered_weights = [weights[c] for c in sorted(weights)]\n",
    "    metric_names = metrics_writer.get_metrics_names(metrics)\n",
    "    for metric in metric_names:\n",
    "        ordered_metric = [metrics[c][metric] for c in sorted(metrics)]\n",
    "        print('%s: %g, 10th percentile: %g, 90th percentile %g' \\\n",
    "              % (metric,\n",
    "                 np.average(ordered_metric, weights=ordered_weights),\n",
    "                 np.percentile(ordered_metric, 10),\n",
    "                 np.percentile(ordered_metric, 90)))\n",
    "    fom = [metrics[c][metric_names[0]] for c in sorted(metrics)]\n",
    "    final = np.average(fom, weights=ordered_weights)\n",
    "    return final\n",
    "\n",
    "class Fedrobust_Trainer:\n",
    "    \n",
    "    def __init__(self, users, groups, train_data, test_data):\n",
    "        self.users = users\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.num_clients_per_round = 0\n",
    "        self.config = []\n",
    "        self.server = []\n",
    "        self.all_clients = []\n",
    "        self.iter = 0\n",
    "        self.acc = 0.0\n",
    "        self._bic = 0.0\n",
    "        self._db_score = 0.0\n",
    "\n",
    "        \n",
    "    def model_config(self, config, dataset, my_model):   \n",
    "        shared_model = my_model\n",
    "        model_path = '%s/%s.py' % (dataset, shared_model)\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please specify a valid dataset and a valid model.')\n",
    "        model_path = '%s.%s' % (dataset, shared_model)\n",
    "\n",
    "        print('############################## %s ##############################' % model_path)\n",
    "        mod = importlib.import_module(model_path)\n",
    "        ClientModel = getattr(mod, 'ClientModel')  \n",
    "        # Suppress tf warnings\n",
    "        tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "        # Create 2 models\n",
    "        model_params = MODEL_PARAMS[model_path]\n",
    "        model_params_list = list(model_params)\n",
    "        model_params_list.insert(0, config[\"seed\"])\n",
    "        model_params_list[1] = config[\"lr\"]\n",
    "        model_params = tuple(model_params_list)\n",
    "        tf.reset_default_graph()\n",
    "        client_model = ClientModel(*model_params)\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Create clients\n",
    "        _users = self.users\n",
    "        groups = [[] for _ in _users]\n",
    "        clients =  [Client(u, g, self.train_data[u], self.test_data[u], client_model) \\\n",
    "                    for u, g in zip(_users, groups)]\n",
    "        print('%d Clients in Total' % len(clients)) \n",
    "        self.all_clients = clients\n",
    "        \n",
    "        if config['poisoning'] == True:\n",
    "            num_agents = int(config[\"num_agents\"] * len(clients)) \n",
    "            clients_per_round = config[\"clients-per-round\"]            \n",
    "            server_ = MDLpoisonServer(client_model, clients, num_agents, model_params_list[2], clients_per_round)\n",
    "        else:\n",
    "            # Create server\n",
    "            server_ = Server(client_model)\n",
    "        self.Server = server_            \n",
    "        return clients, server_, client_model\n",
    "    \n",
    "    def fed_train(self, init_prms, client_in_block):\n",
    "        #server.select_clients(possible_clients, num_clients=len(possible_clients))\n",
    "        #c_ids, c_groups, c_num_samples = server.get_clients_info(None)\n",
    "        eval_every = self.config[\"eval-every\"]\n",
    "        epochs_per_round = self.config['epochs']\n",
    "        batch_size = self.config['batch-size']\n",
    "        print(\"Start training on these clients:\", client_in_block)\n",
    "        block_clients = [self.all_clients[i] for i in client_in_block]\n",
    "        sys_metrics, updates = self.Server.train_model(single_center=init_prms, num_epochs=epochs_per_round, batch_size=batch_size, minibatch=None, clients = block_clients, apply_prox=False)\n",
    "        return sys_metrics, updates\n",
    "         \n",
    "    def fed_update(self):\n",
    "        return self.Server.update_model_nowmode()\n",
    "    \n",
    "    def fed_test(self, nearest_centroid, robust):\n",
    "        accs_ = [0] * len(set(nearest_centroid))\n",
    "        for k,nc in enumerate(set(nearest_centroid)):\n",
    "            cl_within_clus = []\n",
    "            for i, v in enumerate(self.all_clients):\n",
    "                if nearest_centroid[i] == nc:\n",
    "                    cl_within_clus.append(self.all_clients[i])\n",
    "            self.Server.model = robust.centers[nc]\n",
    "            stat_metrics = self.Server.test_model(cl_within_clus)\n",
    "            c_ids, c_groups, c_num_samples = self.Server.get_clients_info(cl_within_clus)\n",
    "            accs_[k] = print_metrics(stat_metrics, c_num_samples)\n",
    "        print(\"--- Acc: \",  np.average(accs_), \" ---\")\n",
    "        log_history(self.iter+1, np.average(accs_), np.average(accs_), cl_within_clus)\n",
    "        self.iter += 1\n",
    "        self.acc = np.average(accs_)\n",
    "\n",
    "    def evaluate(self, rbclf, config):\n",
    "        print(\"Evaluation metrics below: \")\n",
    "        bic = rbclf.sil_score()\n",
    "        db_score = rbclf.DB_score()\n",
    "        print(\"BIC: \", bic)\n",
    "        print(\"DB_score:\", db_score)\n",
    "        return (bic, db_score)\n",
    "        \n",
    "    def begins(self, config, args):\n",
    "        \n",
    "        def shout(text):\n",
    "            return text.upper()\n",
    "        \n",
    "        clients, server, client_model = self.model_config(config, args.dataset, 'cnn')  \n",
    "        \n",
    "        K = config[\"num-clusters\"]\n",
    "        num_rounds = config[\"num-rounds\"]\n",
    "        eval_every = config[\"eval-every\"]\n",
    "        epochs_per_round = config['epochs']\n",
    "        batch_size = config['batch-size']\n",
    "        clients_per_round = config[\"clients-per-round\"]\n",
    "        n_layers = config[args.dataset + \"-num-layers\"]\n",
    "        n_blocks = config[\"num-blocks\"]\n",
    "        coef_ech = config[\"coff-each\"]\n",
    "        \n",
    "        all_ids, all_groups, all_num_samples = server.get_clients_info(clients)\n",
    "        # train all clients one round \n",
    "        _, tmp_data = server.train_model(None, 1, batch_size, None, clients, False)\n",
    "        all_cl_models = [x[1] for x in tmp_data ]\n",
    "        #print(\"the shape of a model is: \", len(all_cl_models[0][1]))\n",
    "        print(\"last layer of a model is:\", all_cl_models[0][n_layers].shape)\n",
    "            \n",
    "        # self.all_cl_models, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n",
    "        robust_helper = KbMOM(X = all_cl_models, K = K, max_iter = num_rounds, nbr_blocks = n_blocks, coef_ech = int(len(clients) * coef_ech) , quantile=0.5, init_type='kmedianpp', n_layers = n_layers)\n",
    "        robust_helper.set_E_func(self.fed_train)\n",
    "        robust_helper.set_M_func(self.fed_update)\n",
    "        print(\"*** Robust algorithm training started ***\")\n",
    "        robust_helper.fit(all_cl_models)\n",
    "        #clients, server, client_model = self.model_config(config, args.dataset, 'cnn_prox')  \n",
    "        centroids = robust_helper.predict(all_cl_models)\n",
    "        self.fed_test(centroids, robust_helper)\n",
    "        \n",
    "        if config[\"benchmark\"] == 1:\n",
    "            vals = self.evaluate(robust_helper, config)\n",
    "            self._bic = vals[0]\n",
    "            self._db_score = vals[1]\n",
    "        return self.acc\n",
    "    \n",
    "    def ends(self):\n",
    "        save_historyfile()\n",
    "        print(\"experiment of Fed Robust is finished.\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36ac2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config rounds:  20\n",
      "config lr:  0.01\n",
      "config epochs:  3\n",
      "config clients per round:  40\n",
      "config poisoning:  False\n",
      "config num agents:  0\n",
      "############################## celeba.cnn ##############################\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/fedrobust.py:91: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:24: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/celeba/cnn.py:20: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/celeba/cnn.py:24: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/jupyter/dev/mlenv/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/celeba/cnn.py:25: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/celeba/cnn.py:26: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/celeba/cnn.py:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:81: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/celeba/cnn.py:36: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:26: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2022-01-31 02:44:24.808025: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-01-31 02:44:24.808056: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-31 02:44:24.808079: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (david-08232021): /proc/driver/nvidia/version does not exist\n",
      "2022-01-31 02:44:24.808278: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2022-01-31 02:44:24.813812: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3100210000 Hz\n",
      "2022-01-31 02:44:24.814141: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555ed72510a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-31 02:44:24.814165: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/utils/tf_utils.py:33: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:32: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:34: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:35: The name tf.profiler.ProfileOptionBuilder is deprecated. Please use tf.compat.v1.profiler.ProfileOptionBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:36: The name tf.profiler.profile is deprecated. Please use tf.compat.v1.profiler.profile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/dev/mlenv/lib/python3.7/site-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/62.41k flops)\n",
      "  conv2d_1/kernel/Initializer/random_uniform (9.22k/18.43k flops)\n",
      "    conv2d_1/kernel/Initializer/random_uniform/mul (9.22k/9.22k flops)\n",
      "    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  conv2d_2/kernel/Initializer/random_uniform (9.22k/18.43k flops)\n",
      "    conv2d_2/kernel/Initializer/random_uniform/mul (9.22k/9.22k flops)\n",
      "    conv2d_2/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  conv2d_3/kernel/Initializer/random_uniform (9.22k/18.43k flops)\n",
      "    conv2d_3/kernel/Initializer/random_uniform/mul (9.22k/9.22k flops)\n",
      "    conv2d_3/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  dense/kernel/Initializer/random_uniform (2.30k/4.61k flops)\n",
      "    dense/kernel/Initializer/random_uniform/mul (2.30k/2.30k flops)\n",
      "    dense/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  conv2d/kernel/Initializer/random_uniform (864/1.73k flops)\n",
      "    conv2d/kernel/Initializer/random_uniform/mul (864/864 flops)\n",
      "    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  batch_normalization/AssignMovingAvg (32/97 flops)\n",
      "    batch_normalization/AssignMovingAvg/mul (32/32 flops)\n",
      "    batch_normalization/AssignMovingAvg/sub_1 (32/32 flops)\n",
      "    batch_normalization/AssignMovingAvg/sub (1/1 flops)\n",
      "  batch_normalization/AssignMovingAvg_1 (32/97 flops)\n",
      "    batch_normalization/AssignMovingAvg_1/mul (32/32 flops)\n",
      "    batch_normalization/AssignMovingAvg_1/sub_1 (32/32 flops)\n",
      "    batch_normalization/AssignMovingAvg_1/sub (1/1 flops)\n",
      "  batch_normalization_1/AssignMovingAvg (32/97 flops)\n",
      "    batch_normalization_1/AssignMovingAvg/mul (32/32 flops)\n",
      "    batch_normalization_1/AssignMovingAvg/sub_1 (32/32 flops)\n",
      "    batch_normalization_1/AssignMovingAvg/sub (1/1 flops)\n",
      "  batch_normalization_1/AssignMovingAvg_1 (32/97 flops)\n",
      "    batch_normalization_1/AssignMovingAvg_1/mul (32/32 flops)\n",
      "    batch_normalization_1/AssignMovingAvg_1/sub_1 (32/32 flops)\n",
      "    batch_normalization_1/AssignMovingAvg_1/sub (1/1 flops)\n",
      "  batch_normalization_2/AssignMovingAvg (32/97 flops)\n",
      "    batch_normalization_2/AssignMovingAvg/mul (32/32 flops)\n",
      "    batch_normalization_2/AssignMovingAvg/sub_1 (32/32 flops)\n",
      "    batch_normalization_2/AssignMovingAvg/sub (1/1 flops)\n",
      "  batch_normalization_2/AssignMovingAvg_1 (32/97 flops)\n",
      "    batch_normalization_2/AssignMovingAvg_1/mul (32/32 flops)\n",
      "    batch_normalization_2/AssignMovingAvg_1/sub_1 (32/32 flops)\n",
      "    batch_normalization_2/AssignMovingAvg_1/sub (1/1 flops)\n",
      "  batch_normalization_3/AssignMovingAvg (32/97 flops)\n",
      "    batch_normalization_3/AssignMovingAvg/mul (32/32 flops)\n",
      "    batch_normalization_3/AssignMovingAvg/sub_1 (32/32 flops)\n",
      "    batch_normalization_3/AssignMovingAvg/sub (1/1 flops)\n",
      "  batch_normalization_3/AssignMovingAvg_1 (32/97 flops)\n",
      "    batch_normalization_3/AssignMovingAvg_1/mul (32/32 flops)\n",
      "    batch_normalization_3/AssignMovingAvg_1/sub_1 (32/32 flops)\n",
      "    batch_normalization_3/AssignMovingAvg_1/sub (1/1 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "98 Clients in Total\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:46: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:42: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "last layer of a model is: (1152, 2)\n",
      "*** Robust algorithm training started ***\n",
      "len of nearest centroid:  3\n",
      "Start training on these clients: [36, 65, 17, 73, 16, 36, 91, 89, 55, 4, 91, 53, 28, 75]\n",
      "Start training on these clients: [74, 79, 59, 79]\n",
      "Start training on these clients: [60, 64, 19, 67, 78, 84, 85, 9, 86, 96, 41, 78, 57, 40, 3, 41, 8, 3, 20, 72, 96]\n",
      "--- Round 2 of 5: Training 39 Clients ---\n",
      "len of nearest centroid:  3\n",
      "Start training on these clients: [47, 11, 56, 45, 76, 92, 49, 29, 71, 39, 38, 69, 21, 24, 18, 23, 27, 30]\n",
      "Start training on these clients: [7, 46, 33, 33]\n",
      "Start training on these clients: [68, 28, 13, 43, 19, 41, 65, 20, 78, 41, 65, 28, 3, 60, 97, 63, 68]\n",
      "--- Round 3 of 5: Training 39 Clients ---\n",
      "len of nearest centroid:  3\n",
      "Start training on these clients: [53, 82, 51, 93, 30, 11, 92, 42, 83, 4, 55, 15]\n",
      "Start training on these clients: [70, 70, 16, 94, 74, 24, 46, 79]\n",
      "Start training on these clients: [72, 0, 19, 62, 6, 88, 86, 50, 26, 34, 3, 20, 19, 0, 19, 20, 20, 0, 75]\n",
      "--- Round 4 of 5: Training 39 Clients ---\n",
      "len of nearest centroid:  3\n",
      "Start training on these clients: [30, 14, 23, 10, 91, 10, 61, 66, 4, 82, 45, 55]\n",
      "Start training on these clients: [59, 33, 5, 95, 5, 89, 44]\n",
      "Start training on these clients: [13, 68, 88, 86, 9, 34, 12, 25, 12, 87, 71, 65, 3, 73, 84, 60, 64, 84, 84, 96]\n",
      "--- Round 5 of 5: Training 39 Clients ---\n",
      "len of nearest centroid:  3\n",
      "Start training on these clients: [56, 23, 93, 92, 37, 92, 30, 14, 55, 10, 53, 55]\n",
      "Start training on these clients: [89, 79, 8, 95, 24, 46]\n",
      "Start training on these clients: [13, 72, 65, 43, 39, 9, 0, 48, 86, 9, 9, 63, 87, 50, 31, 80, 87, 1, 96, 31, 90]\n",
      "accuracy: 0.643629, 10th percentile: 0, 90th percentile 1\n",
      "loss: 0.679631, 10th percentile: 0.260587, 90th percentile 1.49688\n",
      "microf1: 0.643629, 10th percentile: 0, 90th percentile 1\n",
      "macrof1: 0.544937, 10th percentile: 0, 90th percentile 1\n",
      "accuracy: 0.767606, 10th percentile: 0.333333, 90th percentile 1\n",
      "loss: 1.35479, 10th percentile: 0.00115899, 90th percentile 3.63912\n",
      "microf1: 0.767606, 10th percentile: 0.333333, 90th percentile 1\n",
      "macrof1: 0.675117, 10th percentile: 0.25, 90th percentile 1\n",
      "accuracy: 0.615942, 10th percentile: 0.2, 90th percentile 1\n",
      "loss: 0.739315, 10th percentile: 0.212228, 90th percentile 1.28749\n",
      "microf1: 0.615942, 10th percentile: 0.2, 90th percentile 1\n",
      "macrof1: 0.517947, 10th percentile: 0.15, 90th percentile 1\n",
      "--- Acc:  0.6757255955976249  ---\n",
      "experiment of Fed Robust is finished.\n",
      "[67.57255956]\n",
      "CPU times: user 1.34 s, sys: 206 ms, total: 1.54 s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!/home/jupyter/dev/mlenv/bin/python experiment.py -experiment fedrobust -dataset celeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ac928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
