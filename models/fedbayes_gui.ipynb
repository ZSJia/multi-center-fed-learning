{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code block of program file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedbayes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fedbayes.py\n",
    "import copy\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from client import Client\n",
    "from server import Server\n",
    "from model import ServerModel\n",
    "from baseline_constants import MAIN_PARAMS, MODEL_PARAMS\n",
    "from fedbayes_helper import *\n",
    "from fedbayes_tinyhelper import *\n",
    "\n",
    "import metrics.writer as metrics_writer\n",
    "STAT_METRICS_PATH = 'metrics/stat_metrics.csv'\n",
    "SYS_METRICS_PATH = 'metrics/sys_metrics.csv'\n",
    "\n",
    "#from utils.matching.pfnm import layer_group_descent as pdm_multilayer_group_descent\n",
    "from utils.matching.cnn_pfnm import layerwise_sampler\n",
    "from utils.matching.cnn_retrain import reconstruct_weights, local_train, combine_network_after_matching\n",
    "\n",
    "def print_metrics(metrics, weights):\n",
    "    ordered_weights = [weights[c] for c in sorted(weights)]\n",
    "    metric_names = metrics_writer.get_metrics_names(metrics)\n",
    "    for metric in metric_names:\n",
    "        ordered_metric = [metrics[c][metric] for c in sorted(metrics)]\n",
    "        print('%s: %g, 10th percentile: %g, 90th percentile %g' \\\n",
    "              % (metric,\n",
    "                 np.average(ordered_metric, weights=ordered_weights),\n",
    "                 np.percentile(ordered_metric, 10),\n",
    "                 np.percentile(ordered_metric, 90)))\n",
    "\n",
    "class Fedbayes_Sing_Trainer:\n",
    "    \n",
    "    def __init__(self, users, groups, train_data, test_data):\n",
    "        # matching requires num of classes to be set during \n",
    "        # model_config stage, or it can cause program failure\n",
    "        self.users = users\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.num_classes = 0 \n",
    "        self.shape_func = None\n",
    "        self.upd_collector = []\n",
    "    \n",
    "    def recover_weights(self, weights, assignment, model_summary, model_meta_data):\n",
    "        res_weights = []\n",
    "        conv_varname, dense_varname, weight_varname = \"conv\", \"dense\", \"kernel\"\n",
    "        #print(\"checking len, model summ: {}, model meta data: {}\".format(len(model_summary), len(model_meta_data)))\n",
    "        for var_name, o, v in zip(model_summary, model_meta_data, weights):\n",
    "            print(\"name {}, old shape is {}, new shape is {}\".format(var_name, o, v.shape))\n",
    "            if var_name.startswith(conv_varname):\n",
    "                if var_name.endswith(weight_varname):\n",
    "                    w = v.reshape(o)\n",
    "                    w = w.transpose((2, 3, 1, 0))\n",
    "                else:\n",
    "                    w = v\n",
    "            elif var_name.startswith(\"batch\"):\n",
    "                w = np.ones(o)\n",
    "            elif var_name.startswith(dense_varname):\n",
    "                if var_name.endswith(weight_varname):\n",
    "                    w = v.transpose()\n",
    "                else:\n",
    "                    w = v\n",
    "            res_weights.append(w)\n",
    "        # just change last layer, carefully, not sure how it works\n",
    "        # do check the reason out after Jan, find a way to \n",
    "        # improve it.\n",
    "        res_weights[-2] = res_weights[-2].T\n",
    "        return res_weights\n",
    "    \n",
    "    def train_model(self, client_model, train_data, weights, assignment, config):\n",
    "        # what maintain by assignment is a dictionary of \n",
    "        #{layer_name: [global_id]}\n",
    "        # the meaning of it is a worker's all layer(except last) matching assignment\n",
    "        epochs = config[\"epochs\"]\n",
    "        batch_size = config[\"batch-size\"]        \n",
    "        client_model.set_params(weights)\n",
    "        client_model.train(train_data, num_epochs=epochs, batch_size=batch_size)\n",
    "        update = client_model.get_params()\n",
    "        self.upd_collector.append(update)\n",
    "        \n",
    "    def model_config(self, config, dataset, my_model):   \n",
    "        shared_model = my_model\n",
    "        model_path = '%s/%s.py' % (dataset, shared_model)\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please specify a valid dataset and a valid model.')\n",
    "        model_path = '%s.%s' % (dataset, shared_model)\n",
    "\n",
    "        print('############################## %s ##############################' % model_path)\n",
    "        mod = importlib.import_module(model_path)\n",
    "        ClientModel = getattr(mod, 'ClientModel')\n",
    "        self.shape_func = getattr(mod, 'get_convolution_extractor_shape')\n",
    "        # Suppress tf warnings\n",
    "        tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "        # Create 2 models\n",
    "        model_params = MODEL_PARAMS[model_path]\n",
    "        model_params_list = list(model_params)\n",
    "        self.num_classes = model_params[1] # setting num_class to be a member of the trainer\n",
    "        model_params_list.insert(0, config[\"seed\"])\n",
    "        model_params_list[1] = config[\"lr\"]        \n",
    "        model_params = tuple(model_params_list)\n",
    "        tf.reset_default_graph()\n",
    "        client_model = ClientModel(*model_params)\n",
    "\n",
    "        # Create server\n",
    "        server = Server(client_model)\n",
    "\n",
    "        # Create clients\n",
    "        _users = self.users\n",
    "        groups = [[] for _ in _users]\n",
    "        clients =  [Client(u, g, self.train_data[u], self.test_data[u], client_model) \\\n",
    "                    for u, g in zip(_users, groups)]\n",
    "        print('%d Clients in Total' % len(clients)) \n",
    "        return clients, server, client_model\n",
    "    \n",
    "    \n",
    "    def begins(self, config, args):\n",
    "        clients, server, client_model = self.model_config(config, args.dataset, 'cnn')        \n",
    "               \n",
    "        num_rounds = config[\"num-rounds\"]\n",
    "        eval_every = config[\"eval-every\"]\n",
    "        epochs_per_round = config['epochs']\n",
    "        batch_size = config['batch-size']\n",
    "        clients_per_round = config[\"clients-per-round\"]\n",
    "        state_dict = {}\n",
    "        \n",
    "        # Test untrained model on all clients\n",
    "#         stat_metrics = server.test_model(clients)\n",
    "#         all_ids, all_groups, all_num_samples = server.get_clients_info(clients)\n",
    "#         print_metrics(stat_metrics, all_num_samples)\n",
    "\n",
    "    \n",
    "\n",
    "        model_summary = client_model.get_summary()\n",
    "        model_meta_data = client_model.get_meta_data()\n",
    "#         gl_weight = client_model.get_params()\n",
    "        gl_weight = self.batch_BBPMAP(clients[:40], state_dict, client_model, config, args)\n",
    "        gl_weight = self.recover_weights(gl_weight, [], model_summary, model_meta_data)\n",
    "        server.model = gl_weight        \n",
    "        stat_metrics = server.test_model(clients[:40])\n",
    "        all_ids, all_groups, all_num_samples = server.get_clients_info(clients[:40])\n",
    "        print_metrics(stat_metrics, all_num_samples)\n",
    "        first = True\n",
    "               \n",
    "#         for i in range(num_rounds):\n",
    "#             print('--- Round %d of %d: Training %d Clients ---' % (i+1, num_rounds, clients_per_round))\n",
    "            \n",
    "#             server.select_clients(clients, num_clients=clients_per_round)\n",
    "#             batch_clients = server.selected_clients\n",
    "#             if first:\n",
    "#                 cw = gl_weight\n",
    "#             else:\n",
    "#                 cw = self.recover_weights(gl_weight, assignment, model_summary, model_meta_data)\n",
    "#             for k in batch_clients:\n",
    "#                 if first or not (k.id in state_dict):\n",
    "#                     assignment = []\n",
    "#                 else:\n",
    "#                     assignment = state_dict[k.id]\n",
    "#                 self.train_model(client_model, k.train_data, cw, assignment, config)\n",
    "               \n",
    "#             gl_weight = self.batch_BBPMAP(batch_clients, state_dict, client_model, config, args)\n",
    "            \n",
    "#             if (i + 1) % eval_every == 0 or (i + 1) == num_rounds:\n",
    "#                 cw = self.recover_weights(gl_weight, assignment, model_summary, model_meta_data)\n",
    "#                 server.model = cw\n",
    "#                 stat_metrics = server.test_model(clients)\n",
    "#                 print_metrics(stat_metrics, all_num_samples)\n",
    "            \n",
    "#             first = False\n",
    "        client_model.close()\n",
    "        \n",
    "    def ends(self):\n",
    "        print(\"experiment of Fedbayes finished.\")\n",
    "        return\n",
    "\n",
    "    def batch_BBPMAP(self, batch_clients, state_dict, client_model, config, args):\n",
    "        model_summary = client_model.get_summary()        \n",
    "        model_meta_data = client_model.get_meta_data()\n",
    "        \n",
    "        n_classes = self.num_classes\n",
    "#         averaging_weights, cls_freqs = avg_cls_weights(batch_clients, args.dataset, n_classes)\n",
    "        averaging_weights, cls_freqs = avg_cls_weights(args.dataset, n_classes)\n",
    "        sigma=config[\"sigma\"]\n",
    "        sigma0=config[\"sigma0\"]\n",
    "        gamma=config[\"gamma\"]\n",
    "        it = config[\"sample-iter\"]\n",
    "        assignments_list = []\n",
    "        # param names explained:\n",
    "        # C is the number of layers for model structure, no counting bias\n",
    "        # J is the number of clients (workers)\n",
    "        net_list = load_files()\n",
    "        C = int(len(model_meta_data) / 2)\n",
    "        J = len(net_list)\n",
    "        matching_shapes = []\n",
    "        fc_pos = None        \n",
    "\n",
    "        apply_by_j = lambda j: load_local_model_weight_func(j, model_summary)\n",
    "        batch_weights = list(map(apply_by_j, net_list))\n",
    "        batch_freqs = pdm_prepare_freq(cls_freqs, self.num_classes)\n",
    "        \n",
    "        for cur_l in range(1, C):\n",
    "            layer_hungarian_weights, assignment, L_next = layerwise_sampler(\n",
    "                 batch_weights=batch_weights, \n",
    "                 layer_index=cur_l,\n",
    "                 sigma0_layers=sigma0, \n",
    "                 sigma_layers=sigma, \n",
    "                 batch_frequencies=batch_freqs,\n",
    "                 it=it, \n",
    "                 gamma_layers=gamma, \n",
    "                 model_meta_data=model_meta_data,\n",
    "                 model_layer_type= model_summary,\n",
    "                 n_layers= C,\n",
    "                 matching_shapes=matching_shapes,\n",
    "                 )\n",
    "            assignments_list.append(assignment)\n",
    "            for client, a_val in zip(batch_clients, assignment):\n",
    "                p_index = 2 * (cur_l -1)\n",
    "                v_name = model_summary[p_index]\n",
    "                if client.id in state_dict:\n",
    "                    cdict = state_dict[client.id]\n",
    "                else:\n",
    "                    cdict = {}\n",
    "                cdict.update({v_name: a_val})\n",
    "                state_dict.update({client.id : cdict})\n",
    "            print(\"Number of assignment: {}, L_next: {}, matched_weight shape: {} \".format(\n",
    "                len(assignment), L_next, layer_hungarian_weights[0].shape) )\n",
    "\n",
    "            matching_shapes.append(L_next)\n",
    "            temp_network_weg = combine_network_after_matching(batch_weights, cur_l, \n",
    "                                                              model_summary, model_meta_data,\n",
    "                                                              layer_hungarian_weights, L_next, assignment,\n",
    "                                                             matching_shapes, self.shape_func)\n",
    "\n",
    "\n",
    "            old_data = client_model.get_params()\n",
    "            gl_weights = []\n",
    "            for worker in range(J):\n",
    "                j = worker\n",
    "                gl_weights.append(reconstruct_weights(temp_network_weg[j], assignment[j], \n",
    "                                                      model_summary, old_data, \n",
    "                                                      model_summary[2 * cur_l - 2]))\n",
    "\n",
    "            models = local_train(batch_clients, gl_weights, cur_l, config)\n",
    "            batch_weights = list(map(apply_by_j, models))\n",
    "        \n",
    "        ## we handle the last layer carefully here ...\n",
    "        ## averaging the last layer\n",
    "        matched_weights = []\n",
    "        last_layer_weights_collector = []\n",
    "\n",
    "        for worker in range(J):\n",
    "            # firstly we combine last layer's weight and bias\n",
    "            bias_shape = batch_weights[worker][-1].shape\n",
    "            last_layer_bias = batch_weights[worker][-1].reshape((1, bias_shape[0]))\n",
    "            last_layer_weights = np.concatenate((batch_weights[worker][-2].T, last_layer_bias), axis=0)\n",
    "\n",
    "            # the directed normalization doesn't work well, let's try weighted averaging\n",
    "            last_layer_weights_collector.append(last_layer_weights)\n",
    "\n",
    "        last_layer_weights_collector = np.array(last_layer_weights_collector)\n",
    "\n",
    "        avg_last_layer_weight = np.zeros(last_layer_weights_collector[0].shape, dtype=np.float32)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            avg_weight_collector = np.zeros(last_layer_weights_collector[0][:, 0].shape, dtype=np.float32)\n",
    "            for j in range(J):\n",
    "                avg_weight_collector += averaging_weights[j][i]*last_layer_weights_collector[j][:, i]\n",
    "            avg_last_layer_weight[:, i] = avg_weight_collector\n",
    "\n",
    "        #avg_last_layer_weight = np.mean(last_layer_weights_collector, axis=0)\n",
    "        for i in range(C * 2):\n",
    "            if i < (C * 2 - 2):\n",
    "                matched_weights.append(batch_weights[0][i])\n",
    "\n",
    "        matched_weights.append(avg_last_layer_weight[0:-1, :])\n",
    "        matched_weights.append(avg_last_layer_weight[-1, :])\n",
    "        self.upd_collector = []\n",
    "        \n",
    "        return matched_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedbayes_helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fedbayes_helper.py\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from baseline_constants import KERNAL_WIDTH, KERNEL_HEIGHT, NUM_INPUT_CHANNEL, NUM_OUTPUT_CHANNEL\n",
    "\n",
    "# def avg_cls_weights(batch, dataset, num_classes):\n",
    "#     all_class_freq = load_counts(dataset)\n",
    "#     J = len(batch)\n",
    "#     wids = [ c.id for c in batch ]\n",
    "#     wcnt = [ i for i in range(len(batch))]\n",
    "#     ret_class_freq = {c.id: all_class_freq[c.id] for c in batch}\n",
    "\n",
    "#     averaging_weights = np.zeros((J, num_classes), dtype=np.float32)\n",
    "#     for i in range(num_classes):\n",
    "#         total_num_counts = 0\n",
    "#         worker_class_counts = [0] * J\n",
    "#         for j, c in zip(wids, wcnt):\n",
    "#             if i in all_class_freq[j].keys():\n",
    "#                 total_num_counts += all_class_freq[j][i]\n",
    "#                 worker_class_counts[c] = all_class_freq[j][i]\n",
    "#             else:\n",
    "#                 total_num_counts += 0\n",
    "#                 worker_class_counts[c] = 0\n",
    "#         denorm = max(1, total_num_counts)\n",
    "#         if (total_num_counts == 0):\n",
    "#             print(\"Batch clients: {}, classes: {}\".format(wids, i))\n",
    "#         averaging_weights[:, i] = np.array(worker_class_counts) / denorm\n",
    "\n",
    "#     return averaging_weights, ret_class_freq\n",
    "\n",
    "# def saved_cls_counts(clients, file):\n",
    "#     net_cls_counts = {}\n",
    "\n",
    "#     for c in clients:\n",
    "#         unq, unq_cnt = np.unique(c.train_data['y'], return_counts=True)\n",
    "#         tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "#         net_cls_counts[c.id] = tmp\n",
    "        \n",
    "#     with open(file, 'wb+') as f:\n",
    "#         pickle.dump(net_cls_counts, f)\n",
    "\n",
    "# def load_counts(dataset):\n",
    "#     fname = \"{}_counts\".format(dataset)\n",
    "#     with open(fname, 'rb') as f:\n",
    "#         loaded_cls_counts = pickle.load(f) \n",
    "#     return loaded_cls_counts\n",
    "\n",
    "def pdm_prepare_freq(cls_freqs, n_classes=10):\n",
    "    freqs = []\n",
    "    for net_i in sorted(cls_freqs.keys()):\n",
    "        net_freqs = [0] * n_classes\n",
    "\n",
    "        for cls_i in cls_freqs[net_i]:\n",
    "            net_freqs[cls_i] = cls_freqs[net_i][cls_i]\n",
    "\n",
    "        freqs.append(np.array(net_freqs))\n",
    "\n",
    "    return freqs\n",
    "\n",
    "\n",
    "def load_files():\n",
    "    def get_weightfile():\n",
    "        sepath = os.path.join(\".\", \"workernn\")\n",
    "        j_files =  [f for f in os.listdir(sepath) if re.match(\"femnist_.{5}_.{2}\\.pb\", f)]\n",
    "        return j_files\n",
    "\n",
    "    def process_file(f):\n",
    "        fname = os.path.join(\"workernn\", f)\n",
    "        with open(fname, 'rb') as file:\n",
    "            w = pickle.load(file)        \n",
    "        return w \n",
    "    \n",
    "    return list(map(process_file, get_weightfile()))\n",
    "\n",
    "def get_cnn_w(value):\n",
    "    o_shape = value.shape\n",
    "    width, height = o_shape[KERNAL_WIDTH], o_shape[KERNEL_HEIGHT]\n",
    "    num_in_chn, num_out_chn = o_shape[NUM_INPUT_CHANNEL], o_shape[NUM_OUTPUT_CHANNEL]\n",
    "    n_shape = (width * height * num_in_chn, num_out_chn)    \n",
    "    w = value.reshape(n_shape)\n",
    "    # IMPORTANAT, here need to invoke transpose , because\n",
    "    # in orignal paper, they use pytorch, and the order in \n",
    "    # pytorch is different from tensorflow\n",
    "    # the order is (NUM_OUTPUT_CHANNEL, NUM_INPUT_CHANNEL, ker_width, ker_height)\n",
    "    w = w.transpose() \n",
    "    return w\n",
    "   \n",
    "def load_local_model_weight_func(model, model_summary):\n",
    "    all_layers = []\n",
    "    dense_varname, weight_varname = \"dense\", \"kernel\"\n",
    "    for var_name, value in zip(model_summary, model):\n",
    "        if var_name.startswith(\"conv\"):\n",
    "            if var_name.endswith(weight_varname):\n",
    "                all_layers.append(get_cnn_w(value))\n",
    "            else:\n",
    "                all_layers.append(value)\n",
    "        elif var_name.startswith(\"batch\"):\n",
    "            pass\n",
    "        elif var_name.startswith(dense_varname):\n",
    "            if var_name.endswith(weight_varname):\n",
    "                all_layers.append(value.transpose())\n",
    "            else:\n",
    "                all_layers.append(value)\n",
    "    return all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def pdm_prepare_freq(cls_freqs, n_classes=10):\n",
    "    freqs = []\n",
    "\n",
    "    for net_i in sorted(cls_freqs.keys()):\n",
    "        net_freqs = [0] * n_classes\n",
    "\n",
    "        for cls_i in cls_freqs[net_i]:\n",
    "            net_freqs[cls_i] = cls_freqs[net_i][cls_i]\n",
    "\n",
    "        freqs.append(np.array(net_freqs))\n",
    "\n",
    "    return freqs\n",
    "\n",
    "with open('fems_counts.pb', 'rb') as f:\n",
    "  loaded_cls_counts = pickle.load(f)\n",
    "\n",
    "batch_freqs = pdm_prepare_freq(loaded_cls_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_weights():\n",
    "    batchws = []\n",
    "    for j in range(num_workers):\n",
    "        with open('mni_workernn_{}'.format(j), 'rb') as f:\n",
    "            ws = pickle.load(f)\n",
    "            batchws.append(ws)\n",
    "    \n",
    "    return batchws\n",
    "\n",
    "worker_batches = prepare_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdm_matching_multilayer(it=0, sigma0=None, sigma=None, gamma=None):\n",
    "        print(\"Gamma: \", gamma, \"Sigma: \", sigma, \"Sigma0: \", sigma0)\n",
    "\n",
    "        hungarian_weights = pdm_multilayer_group_descent(\n",
    "            worker_batches, sigma0_layers=sigma0, sigma_layers=sigma, batch_frequencies=batch_freqs, it=it, gamma_layers=gamma\n",
    "        )\n",
    "        \n",
    "        return hungarian_weights\n",
    "\n",
    "#         train_acc, test_acc, _, _ = compute_pdm_net_accuracy(hungarian_weights, train_dl, test_dl, n_classes)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the global model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "def create_mlp(num_neurons):\n",
    "  model = models.Sequential()\n",
    "  model.add(tf.keras.Input(shape=(28,28)))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(num_neurons, activation='relu'))\n",
    "  model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "      metrics=['accuracy']\n",
    "  ) \n",
    "\n",
    "  return model \n",
    "\n",
    "weights = compute_pdm_matching_multilayer(10, 7.0, 1.0, 2.0)\n",
    "num_gl_neurons = weights[0].shape[1]\n",
    "gl_modl = create_mlp(num_gl_neurons)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "gl_modl.set_weights(weights)\n",
    "history = gl_modl.evaluate(x_test, y_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My code snippet for computing distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_n_layers = int(len(worker_batches[0]) / 2)\n",
    "t_n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_batches[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_batches[0][0].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_pdm_matching_multilayer(5, 1.0, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the cost matrix computed by PFNM's code, I export it to file\n",
    "with open('cost_matrix.pb', 'rb') as f:\n",
    "    last_one = pickle.load(f)\n",
    "    \n",
    "last_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "%timeit -n10 -r5 linear_sum_assignment(last_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lapsolver import solve_dense\n",
    "\n",
    "%timeit -n10 -r5 solve_dense(last_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ind_sk, col_ind_sk = linear_sum_assignment(last_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ind_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_one[row_ind_sk, col_ind_sk].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the way to interpre the result, row_ind and col_ind\n",
    "# is that row_ind give you what is the task, the col_ind give \n",
    "# you who (from input matrix) is the one assgined this task\n",
    "# i.e. the old row_ind in input_matrix is the value of col_ind\n",
    "cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "row_ind, col_ind = linear_sum_assignment(cost)\n",
    "col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a57dee63f027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'9'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"workernn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cifar10_{}.pb\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "j = '9'\n",
    "path = os.path.join(\"workernn\", \"cifar10_{}.pb\".format(j))\n",
    "with open(path, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "    \n",
    "# weights[2] = weights[2].reshape((-1, 64))   \n",
    "\n",
    "for w in weights:\n",
    "    print(w.shape)\n",
    "    \n",
    "#weights_bias = [np.hstack((batch_weights[j][2 * layer_index - 2], batch_weights[j][2 * layer_index - 1].reshape(-1, 1))) for j in range(J)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedbayes_tinyhelper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fedbayes_tinyhelper.py\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def load_counts(dataset):\n",
    "    fname = \"{}_counts\".format(dataset)\n",
    "    with open(fname, 'rb') as f:\n",
    "        loaded_cls_counts = pickle.load(f) \n",
    "    return loaded_cls_counts\n",
    "\n",
    "def avg_cls_weights(dataset, num_classes):\n",
    "    all_class_freq = load_counts(dataset)\n",
    "    J = 40\n",
    "    keys = list(all_class_freq)\n",
    "    random.shuffle(keys)\n",
    "\n",
    "    averaging_weights = np.zeros((J, num_classes), dtype=np.float32)\n",
    "    for i in range(num_classes):\n",
    "        total_num_counts = 0\n",
    "        worker_class_counts = [0] * J\n",
    "        for j in range(J):\n",
    "            w = keys[j]\n",
    "            if i in all_class_freq[w].keys():\n",
    "                total_num_counts += all_class_freq[w][i]\n",
    "                worker_class_counts[j] = all_class_freq[w][i]\n",
    "            else:\n",
    "                total_num_counts += 0\n",
    "                worker_class_counts[j] = 0\n",
    "        averaging_weights[:, i] = worker_class_counts / total_num_counts\n",
    "\n",
    "    return averaging_weights, all_class_freq\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdm_prepare_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-560caf88b67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcls_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbatch_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdm_prepare_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_freqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdm_prepare_freq' is not defined"
     ]
    }
   ],
   "source": [
    "cls_counts = load_counts('fem')\n",
    "batch_freqs = pdm_prepare_freq(cls_counts)\n",
    "print(batch_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
