{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daef8684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85a1f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting robust_main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile robust_main.py\n",
    "import numpy as np\n",
    "import random\n",
    "from math import modf, log\n",
    "from scipy.spatial.distance import cdist\n",
    "from kbmom.kmedianpp import euclidean_distances, kmedianpp_init\n",
    "\n",
    "class KbMOM:\n",
    "    \n",
    "    def __init__(self,X,K,nbr_blocks,coef_ech = 6,max_iter = 10,outliers = None, confidence = 0.95, threshold = 0.001,quantile   = 0.5,initial_centers = None,init_type ='km++',averaging_strategy='cumul', n_layers = 1):\n",
    "        '''\n",
    "        # X             : numpy array = contains the data we want to cluster\n",
    "        # K             : number of clusters\n",
    "        # nbr_blocks    : number of blocks to create in init and loop\n",
    "        # coef_ech      : NUMBER of data in each block and cluster\n",
    "        # quantile      : quantile to keep for the empirical risk; by default the median\n",
    "        # max_iter      : number of iterations of the algorithm\n",
    "        # max_iter_init : number of iterations to run for the kmeans in the initilization procedure\n",
    "        # kmeanspp      : boolean. If true then init by kmeanspp else kmedianpp\n",
    "        # outliers      : number of supposed outliers\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # the structure of X_blocks and centers needs to to be modified.\n",
    "        # they need to store the whole parameters of a model.\n",
    "        # but when computing distance or predicting, just use the last\n",
    "        # layer of the parameters.\n",
    "        # to faciliate this computing, just added a function last_layer\n",
    "        \n",
    "        # Thus each item in X is a [n_layers] model, X is by number of clients array of numpy data.\n",
    "        '''\n",
    "        \n",
    "        # given element\n",
    "        self.X          = None\n",
    "        self.K          = K\n",
    "        self.max_iter   = max_iter\n",
    "        self.n, self.p  = len(X), X[0][n_layers].shape[0] * X[0][n_layers].shape[1]\n",
    "        self.quantile   = quantile\n",
    "        self.coef_ech   = coef_ech\n",
    "        self.B          = nbr_blocks\n",
    "        self.alpha      = 1 - confidence\n",
    "        self.threshold  = threshold\n",
    "        self.init_type  = init_type\n",
    "        self.averaging_strategy = averaging_strategy\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        \n",
    "        # Test some given values\n",
    "        if outliers is not None:\n",
    "            self.outliers = outliers\n",
    "            t_sup = self.bloc_size(self.n,self.outliers)\n",
    "            if self.coef_ech > t_sup:\n",
    "                self.coef_ech  = max((t_sup-5),1)\n",
    "                self.coef_ech  = int(round(self.coef_ech))\n",
    "                print('warning:: the size of blocks has been computed according to the breakdown point theory')\n",
    "\n",
    "            B_sup = self.bloc_nb(self.n,self.outliers,b_size=self.coef_ech,alpha=self.alpha)\n",
    "            if self.B < B_sup :\n",
    "                self.B     = round(B_sup) + 10\n",
    "                self.B     = int(self.B)\n",
    "                print('warning:: the number of blocks has been computed according to the breakdown point theory')\n",
    "        \n",
    "        # Deal with exceptions:\n",
    "        if self.coef_ech <= self.K:\n",
    "            self.coef_ech = 2*self.K\n",
    "        \n",
    "        # internal element initialization\n",
    "        self.score         = np.ones((self.n,))\n",
    "        \n",
    "        if isinstance(initial_centers,np.ndarray):\n",
    "            self.centers = initial_centers\n",
    "        else:\n",
    "            self.centers = 0\n",
    "            \n",
    "        self.block_empirical_risk = []\n",
    "        self.median_block_centers = []\n",
    "        self.empirical_risk = []\n",
    "        self.iter           = 1\n",
    "        self.warnings       = 'None'\n",
    "    \n",
    "    def init_centers_function(self,X,idx_blocks):\n",
    "        '''\n",
    "        # Initialisation function: create nbr_blocks blocks, initialize with a kmeans++, \n",
    "        retrieve the index of the median block and its empirical risk value\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : numpy array of data\n",
    "        . idx_blocks : list of indices contained in the B blocks\n",
    "        '''\n",
    "        \n",
    "        # Blocks creation\n",
    "        size_of_blocks = self.coef_ech\n",
    "        \n",
    "        block_inertia = []\n",
    "        init_centers  = []\n",
    "        if self.init_type=='km++':\n",
    "            # instanciation of kmeans++\n",
    "            x_squared = X**2\n",
    "            x_squared_norms = x_squared.sum(axis=1)\n",
    "        \n",
    "            for idx_ in idx_blocks: \n",
    "                init_centers_ = kmedianpp_init(X[idx_,:], self.K, x_squared_norms[idx_], n_local_trials=None, square=True)\n",
    "                init_centers.append(init_centers_)\n",
    "                block_inertia.append(self.inertia_function(idx_,init_centers_))\n",
    "        else:\n",
    "            for idx_ in idx_blocks: \n",
    "                init_centers_ = self.random_init([X[i] for i in idx_])\n",
    "                init_centers.append(init_centers_)\n",
    "                block_inertia.append(self.inertia_function(idx_,init_centers_))\n",
    "            \n",
    "        median_risk = sorted(block_inertia)[round(self.quantile*len(block_inertia))]\n",
    "\n",
    "        # Select the Q-quantile bloc\n",
    "        id_median = block_inertia.index(median_risk)\n",
    "        \n",
    "        # init centers\n",
    "        self.centers = init_centers[id_median]\n",
    "        \n",
    "        return(id_median,median_risk)\n",
    "    \n",
    "    def random_init(self,dataset):\n",
    "        rnd_ =  np.random.choice(len(dataset), self.K)\n",
    "        s = [dataset[i] for i in rnd_]\n",
    "        return s\n",
    "        \n",
    "    def sampling_all_blocks_function(self):#,nbr_blocks,weighted_point,cluster_sizes):\n",
    "        '''\n",
    "        # Function which creates nbr_blocks blocks based on self.coef_ech and self.B\n",
    "        '''\n",
    "        blocks = [random.choices(np.arange(self.n),k = self.coef_ech) for i in range(self.B)]\n",
    "        return(blocks)\n",
    "    \n",
    "    \n",
    "    def inertia_function(self,idx_block,centroids = None):\n",
    "        '''\n",
    "        # Function which computes empirical risk per block\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : numpy array of data\n",
    "        . idx_block  : list of indices contained in the B blocks\n",
    "        . centroids  : if not None get the centers from kmeans++ initialisation\n",
    "        '''\n",
    "        if not isinstance(centroids,list):\n",
    "            centroids = self.centers\n",
    "        \n",
    "#         print(\"The block contains:[\",  idx_block ,\"]\")\n",
    "        X_block           = [self.X[i] for i in idx_block]\n",
    "        nearest_centroid  = self.fed_dist(X_block,centroids,'sqeuclidean').argmin(axis=1)\n",
    "        \n",
    "        if len(set(nearest_centroid)) == self.K and sum(np.bincount(nearest_centroid) > 1) == self.K :\n",
    "            within_group_inertia = 0\n",
    "            for k,nc in enumerate(set(nearest_centroid)):\n",
    "                within_group_inertia += self.inertia_per_cluster(X_block, nearest_centroid, nc)\n",
    "            \n",
    "            return(within_group_inertia/len(idx_block))\n",
    "        else:\n",
    "            return(-1)\n",
    "     \n",
    "            \n",
    "    def median_risk_function(self,X,blocks):\n",
    "        '''\n",
    "        # Function which computes the sum of all within variances and return the index of the median block\n",
    "        and its empirical risk\n",
    "        \n",
    "        ```parameters ```       \n",
    "            . blocks     : list of indices forming the blocks\n",
    "            . X          : matrix of datapoints\n",
    "        '''\n",
    "        \n",
    "        block_inertia = list(map(self.inertia_function, blocks))\n",
    "            \n",
    "        nb_nonvalide_blocks = sum(np.array(block_inertia) == -1)\n",
    "        nb_valide_blocks    = int(self.B - nb_nonvalide_blocks)\n",
    "        \n",
    "        if nb_nonvalide_blocks != self.B:\n",
    "            \n",
    "            median_risk = sorted(block_inertia)[nb_nonvalide_blocks:][round(self.quantile*nb_valide_blocks)]\n",
    "            \n",
    "            # Select the Q-quantile bloc\n",
    "            id_median = block_inertia.index(median_risk)\n",
    "            return(id_median,median_risk)\n",
    "    \n",
    "        else:\n",
    "            return(None,None)\n",
    "        \n",
    "    def medianblock_centers_function(self,X,id_median,blocks):\n",
    "        '''\n",
    "        #compute the barycenter of each cluster in the median block\n",
    "        \n",
    "         ``` prms ```\n",
    "         . blocks     : list of indices forming the blocks\n",
    "         . X          : matrix of datapoints\n",
    "         . id_median  : index of the median block\n",
    "        '''\n",
    "        X_block           = [X[i] for i in blocks[id_median]]\n",
    "        distances         = self.fed_dist(X_block,self.centers,'sqeuclidean')\n",
    "        nearest_centroid  = distances.argmin(axis=1)\n",
    " \n",
    "        print(\"len of nearest centroid: \", len(set(nearest_centroid)))\n",
    "        centers_ = [0] * len(set(nearest_centroid))\n",
    "        for k,nc in enumerate(set(nearest_centroid)):\n",
    "            cl_block = []\n",
    "            for i,v  in  enumerate( blocks[id_median] ) :\n",
    "                if nearest_centroid[i] == nc:\n",
    "                    cl_block.append(v)\n",
    "            _, upd = self.E_func(self.centers[nc], cl_block)\n",
    "            centers_[k] = self.M_func()\n",
    "            cnt = 0\n",
    "            for i, v in enumerate( blocks[id_median] ):\n",
    "                if nearest_centroid[i] == nc:\n",
    "                    self.X[v] = upd[cnt][1] # update is a tuple which return by E_func, the 0 is a number of samples, the 1 is model\n",
    "                    cnt += 1\n",
    "                             \n",
    "        self.centers = centers_\n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def weigthingscheme(self,median_block):\n",
    "        '''\n",
    "        Function which computes data depth\n",
    "        \n",
    "        ``` prms ```\n",
    "        . median_block: list containing the indices of data in the median block\n",
    "        \n",
    "        ''' \n",
    "        for idk in median_block:\n",
    "            self.score[idk] += 1\n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def fit(self,X):\n",
    "        '''\n",
    "        # Main loop of the K-bmom algorithm:\n",
    "        \n",
    "         ``` prms ```\n",
    "        . X          : matrix of datapoints \n",
    "        '''\n",
    "        self.X = X\n",
    "        # initialisation step\n",
    "        if not isinstance(self.centers,np.ndarray):\n",
    "            idx_block = self.sampling_all_blocks_function()\n",
    "            id_median , median_risk_ = self.init_centers_function(X,idx_block)\n",
    "\n",
    "            self.block_empirical_risk.append(median_risk_)\n",
    "            self.medianblock_centers_function(X, id_median,idx_block)\n",
    "            self.median_block_centers.append(self.centers)\n",
    "            self.empirical_risk.append(sum(self.fed_dist(self.X, self.centers,'sqeuclidean').min(axis=1))/self.n)\n",
    "            self.weigthingscheme(median_block=idx_block[id_median])\n",
    "        \n",
    "        if self.averaging_strategy == 'cumul':\n",
    "            cumul_centers_ = self.centers\n",
    "        \n",
    "        # Main Loop - fitting process\n",
    "        print(\"**** Main Loop Fitting ****\")\n",
    "        if (self.max_iter == 0):\n",
    "            condition = False\n",
    "        else:\n",
    "            condition = True\n",
    "       \n",
    "        while condition:\n",
    "\n",
    "            # sampling\n",
    "            idx_block = self.sampling_all_blocks_function()\n",
    "            \n",
    "            # Compute empirical risk for all blocks and select the empirical-block\n",
    "            id_median , median_risk_ = self.median_risk_function(self.X,idx_block)\n",
    "            \n",
    "            # If blocks are undefined, then restarting strategy\n",
    "            loop_within = 0\n",
    "            while (id_median == None) and loop_within < 10:\n",
    "                idx_block = self.sampling_all_blocks_function()\n",
    "                id_median , median_risk_ = self.init_centers_function(self.X,idx_block)\n",
    "                cumul_centers_  = np.zeros((self.K,self.p))\n",
    "                self.warnings = 'restart'\n",
    "                loop_within += 1\n",
    "            \n",
    "            if id_median == None:\n",
    "                self.iter = self.max_iter\n",
    "                self.warnings = 'algorithm did not converge'\n",
    "                condition = False\n",
    "                \n",
    "            else:\n",
    "                # update all parameters\n",
    "                self.block_empirical_risk.append(median_risk_)\n",
    "                self.medianblock_centers_function(self.X,id_median,idx_block)\n",
    "                self.median_block_centers.append(self.centers)\n",
    "                self.empirical_risk.append(sum(self.fed_dist(self.X,self.centers,'sqeuclidean').min(axis=1))/self.n)\n",
    "                self.weigthingscheme(median_block=idx_block[id_median])\n",
    "\n",
    "#                 if self.averaging_strategy == 'cumul' and self.iter > (self.max_iter - 10):\n",
    "#                     decay = self.max_iter - 10\n",
    "#                     #current_centers = self.pivot(self.centers,cumul_centers_)\n",
    "#                     cumul_centers_  = (self.centers / (self.iter - decay)) + (self.iter-decay - 1)/(self.iter - decay) * cumul_centers_\n",
    "#                     self.centers = cumul_centers_\n",
    "\n",
    "                self.iter += 1\n",
    "                if self.iter>=self.max_iter:\n",
    "                    condition = False\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Function which computes the partition based on the centroids of Median Block \n",
    "        '''\n",
    "        D_nk = self.fed_dist(X,self.centers,'sqeuclidean')\n",
    "        return(D_nk.argmin(axis=1))\n",
    "    \n",
    "\n",
    "    def bloc_size(self,n_sample,n_outliers):\n",
    "        '''\n",
    "        Function which fits the maximum size of blocks before a the breakpoint\n",
    "        ```prms```\n",
    "        n_sample: nb of data\n",
    "        n_outlier: nb of outliers\n",
    "        '''\n",
    "        return(log(2.)/log(1/(1- (n_outliers/n_sample))))\n",
    "\n",
    "\n",
    "    def bloc_nb(self,n_sample,n_outliers,b_size=None,alpha=0.05):\n",
    "        '''\n",
    "        Function which fits the minimum nb of blocks for a given size t before a the breakpoint\n",
    "        ```prms```\n",
    "        n_sample: nb of data\n",
    "        n_outlier: nb of outliers\n",
    "        b_size = bloc_size\n",
    "        alpha : threshold confiance\n",
    "        '''\n",
    "        if n_outliers/n_sample >= 0.5:\n",
    "            print('too much noise')\n",
    "            return()\n",
    "        elif b_size == None:\n",
    "            t = bloc_size(n_sample,n_outliers)\n",
    "            return(log(1/alpha) / (2* ((1-n_outliers/n_sample)**t - 1/2)**2))\n",
    "        else:\n",
    "            t = b_size\n",
    "            return(log(1/alpha) / (2* ((1-n_outliers/n_sample)**t - 1/2)**2))\n",
    "   \n",
    "    def stopping_crit(self,risk_median):\n",
    "        risk_ = risk_median[::-1][:3]\n",
    "        den = (risk_[2]-risk_[1])-(risk_[1]-risk_[0])\n",
    "        Ax = risk_[2] - (risk_[2]-risk_[1])**2/den\n",
    "        return(Ax)\n",
    "    \n",
    "    def stopping_crit_GMM(self,risk_median):\n",
    "        risk_ = risk_median[::-1][:3]\n",
    "        Aq   = (risk_[0] - risk_[1])/(risk_[1] - risk_[2])\n",
    "        \n",
    "        Rinf = risk_[1] + 1/(1-Aq)*(risk_[0] - risk_[1])\n",
    "        return(Rinf)\n",
    "        \n",
    "    def pivot(self,mu1,mu2):\n",
    "        error    = cdist(mu1,mu2).argmin(axis=1)\n",
    "        pivot_mu = np.zeros((self.K,self.p))\n",
    "        for i,j in enumerate(error):\n",
    "            pivot_mu[i,:] = mu1[j,:]\n",
    "        return(pivot_mu)\n",
    "    \n",
    "    def set_E_func(self, func):\n",
    "        self.E_func  = func\n",
    "        \n",
    "    def set_M_func(self, func):\n",
    "        self.M_func = func\n",
    "        \n",
    "    def last_layer(self, X_list):\n",
    "        return [x[self.n_layers] for x in X_list]\n",
    "    \n",
    "    def fed_dist(self, Xa, Xb, method = 'sqeuclidean'):\n",
    "        xa_transformed = self.last_layer(Xa)\n",
    "        xb_transformed = self.last_layer(Xb)\n",
    "        \n",
    "        xa = list(map(lambda x: x.flatten(), xa_transformed))\n",
    "        xb = list(map(lambda x: x.flatten(), xb_transformed))\n",
    "            \n",
    "        return cdist(np.array(xa), np.array(xb), method)\n",
    "    \n",
    "    def inertia_per_cluster(self, X_block, nearest_centroids, nc):\n",
    "        clster = list()\n",
    "        tran_x_block = self.last_layer(X_block)\n",
    "        for i, xb in enumerate(tran_x_block):\n",
    "            if nearest_centroids[i] == nc:\n",
    "                clster.append(xb.flatten())\n",
    "                \n",
    "        centers_ = np.array(clster).mean(axis = 0).reshape(1, -1)\n",
    "        return cdist(np.array(clster), centers_,'sqeuclidean').sum()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0816320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedrobust.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fedrobust.py\n",
    "# note to audience, fedrobust is based on the work of Median-of-means K-means,\n",
    "# author is Camille Saumard, his email is camille.brunet@gmail.com\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from math import modf, log\n",
    "from scipy.spatial.distance import cdist\n",
    "from kbmom.kmedianpp import euclidean_distances, kmedianpp_init\n",
    "\n",
    "# tensorflow is required for our experiment, please install tf 1.5 not 2\n",
    "import tensorflow as tf\n",
    "import metrics.writer as metrics_writer\n",
    "\n",
    "from baseline_constants import MAIN_PARAMS, MODEL_PARAMS\n",
    "from client import Client\n",
    "from server import Server\n",
    "from model import ServerModel\n",
    "from utils.constants import DATASETS\n",
    "from robust_main import KbMOM\n",
    "\n",
    "\n",
    "STAT_METRICS_PATH = 'metrics/stat_metrics.csv'\n",
    "SYS_METRICS_PATH = 'metrics/sys_metrics.csv'\n",
    "\n",
    "def online(clients):\n",
    "    \"\"\"We assume all users are always online.\"\"\"\n",
    "    return clients\n",
    "\n",
    "def save_model(server_model, dataset, model):\n",
    "    \"\"\"Saves the given server model on checkpoints/dataset/model.ckpt.\"\"\"\n",
    "    # Save server model\n",
    "    ckpt_path = os.path.join('checkpoints', dataset)\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "    save_path = server_model.save(os.path.join(ckpt_path, '%s.ckpt' % model))\n",
    "    print('Model saved in path: %s' % save_path)\n",
    "\n",
    "\n",
    "def print_metrics(metrics, weights):\n",
    "    ordered_weights = [weights[c] for c in sorted(weights)]\n",
    "    metric_names = metrics_writer.get_metrics_names(metrics)\n",
    "    for metric in metric_names:\n",
    "        ordered_metric = [metrics[c][metric] for c in sorted(metrics)]\n",
    "        print('%s: %g, 10th percentile: %g, 90th percentile %g' \\\n",
    "              % (metric,\n",
    "                 np.average(ordered_metric, weights=ordered_weights),\n",
    "                 np.percentile(ordered_metric, 10),\n",
    "                 np.percentile(ordered_metric, 90)))\n",
    "    fom = [metrics[c][metric_names[0]] for c in sorted(metrics)]\n",
    "    final = np.average(fom, weights=ordered_weights)\n",
    "    return final\n",
    "\n",
    "class Fedrobust_Trainer:\n",
    "    \n",
    "    def __init__(self, users, groups, train_data, test_data):\n",
    "        self.users = users\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.num_clients_per_round = 0\n",
    "        self.config = []\n",
    "        self.server = []\n",
    "        self.all_clients = []\n",
    "\n",
    "        \n",
    "    def model_config(self, config, dataset, my_model):   \n",
    "        shared_model = my_model\n",
    "        model_path = '%s/%s.py' % (dataset, shared_model)\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please specify a valid dataset and a valid model.')\n",
    "        model_path = '%s.%s' % (dataset, shared_model)\n",
    "\n",
    "        print('############################## %s ##############################' % model_path)\n",
    "        mod = importlib.import_module(model_path)\n",
    "        ClientModel = getattr(mod, 'ClientModel')  \n",
    "        # Suppress tf warnings\n",
    "        tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "        # Create 2 models\n",
    "        model_params = MODEL_PARAMS[model_path]\n",
    "        model_params_list = list(model_params)\n",
    "        model_params_list[0] = config[\"lr\"]\n",
    "        model_params = tuple(model_params_list)\n",
    "        tf.reset_default_graph()\n",
    "        client_model = ClientModel(config[\"seed\"], *model_params)\n",
    "\n",
    "        # Create server\n",
    "        server_ = Server(client_model)\n",
    "        self.Server = server_\n",
    "        self.config = config\n",
    "\n",
    "        # Create clients\n",
    "        _users = self.users\n",
    "        groups = [[] for _ in _users]\n",
    "        clients =  [Client(u, g, self.train_data[u], self.test_data[u], client_model) \\\n",
    "                    for u, g in zip(_users, groups)]\n",
    "        print('%d Clients in Total' % len(clients)) \n",
    "        self.all_clients = clients\n",
    "        return clients, server_, client_model\n",
    "    \n",
    "    def fed_train(self, init_prms, client_in_block):\n",
    "        #server.select_clients(possible_clients, num_clients=len(possible_clients))\n",
    "        #c_ids, c_groups, c_num_samples = server.get_clients_info(None)\n",
    "        eval_every = self.config[\"eval-every\"]\n",
    "        epochs_per_round = self.config['epochs']\n",
    "        batch_size = self.config['batch-size']\n",
    "        print(\"Start training on these clients:\", client_in_block)\n",
    "        block_clients = [self.all_clients[i] for i in client_in_block]\n",
    "        sys_metrics, updates = self.Server.train_model(single_center=init_prms, num_epochs=epochs_per_round, batch_size=batch_size, minibatch=None, clients = block_clients, apply_prox=False)\n",
    "        return sys_metrics, updates\n",
    "         \n",
    "    def fed_update(self):\n",
    "        return self.Server.update_model_nowmode()\n",
    "    \n",
    "    def begins(self, config, args):\n",
    "        \n",
    "        def shout(text):\n",
    "            return text.upper()\n",
    "        \n",
    "        clients, server, client_model = self.model_config(config, args.dataset, 'cnn')  \n",
    "        \n",
    "        K = config[\"num-clusters\"]\n",
    "        num_rounds = config[\"num-rounds\"]\n",
    "        eval_every = config[\"eval-every\"]\n",
    "        epochs_per_round = config['epochs']\n",
    "        batch_size = config['batch-size']\n",
    "        clients_per_round = config[\"clients-per-round\"]\n",
    "        n_layers = config[\"num-layers\"]\n",
    "        \n",
    "        all_ids, all_groups, all_num_samples = server.get_clients_info(clients)\n",
    "        # train all clients one round \n",
    "        _, tmp_data = server.train_model(None, 1, batch_size, None, clients, False)\n",
    "        all_cl_models = [x[1] for x in tmp_data ]\n",
    "        #print(\"the shape of a model is: \", len(all_cl_models[0][1]))\n",
    "        print(\"last layer of a model is:\", all_cl_models[0][n_layers].shape)\n",
    "            \n",
    "        # self.all_cl_models, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n",
    "        #robust_helper = KbMOM(X = all_cl_models, K = K, nbr_blocks = 20, coef_ech = int(len(clients) * 0.33) , quantile=0.5, init_type='kmedianpp', n_layers = n_layers)\n",
    "        #robust_helper.set_E_func(self.fed_train)\n",
    "        #robust_helper.set_M_func(self.fed_update)\n",
    "        #print(\"*** Robust algorithm training started ***\")\n",
    "        #robust_helper.fit(all_cl_models)\n",
    "        #clients, server, client_model = self.model_config(config, args.dataset, 'cnn_prox')  \n",
    "\n",
    "        return 0.5\n",
    "    \n",
    "    def ends(self):\n",
    "        print(\"experiment of Fed Robust is finished.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c947e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config rounds:  20\n",
      "config lr:  0.01\n",
      "config epochs:  5\n",
      "config clients per round:  40\n",
      "############################## femnist.cnn ##############################\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/fedrobust.py:87: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:24: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/femnist/cnn.py:43: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/femnist/cnn.py:52: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/jupyter/dev/mlenv/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/femnist/cnn.py:53: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/femnist/cnn.py:62: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/femnist/cnn.py:68: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/dev/mlenv/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:81: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/femnist/cnn.py:71: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:26: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2022-01-11 04:18:26.151660: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-01-11 04:18:26.152661: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-11 04:18:26.152726: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (david-08232021): /proc/driver/nvidia/version does not exist\n",
      "2022-01-11 04:18:26.153704: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-01-11 04:18:26.253339: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "2022-01-11 04:18:26.255025: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a1d09a8f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-11 04:18:26.255079: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/utils/tf_utils.py:33: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:32: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:34: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:35: The name tf.profiler.ProfileOptionBuilder is deprecated. Please use tf.compat.v1.profiler.ProfileOptionBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:36: The name tf.profiler.profile is deprecated. Please use tf.compat.v1.profiler.profile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/dev/mlenv/lib/python3.7/site-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/1.69m flops)\n",
      "  dense/kernel/Initializer/random_uniform (777.73k/1.56m flops)\n",
      "    dense/kernel/Initializer/random_uniform/mul (777.73k/777.73k flops)\n",
      "    dense/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  conv2d_1/kernel/Initializer/random_uniform (51.20k/102.40k flops)\n",
      "    conv2d_1/kernel/Initializer/random_uniform/mul (51.20k/51.20k flops)\n",
      "    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  dense_1/kernel/Initializer/random_uniform (15.38k/30.75k flops)\n",
      "    dense_1/kernel/Initializer/random_uniform/mul (15.38k/15.38k flops)\n",
      "    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  conv2d/kernel/Initializer/random_uniform (800/1.60k flops)\n",
      "    conv2d/kernel/Initializer/random_uniform/mul (800/800 flops)\n",
      "    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)\n",
      "  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)\n",
      "  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)\n",
      "  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:46: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "190 Clients in Total\n",
      "WARNING:tensorflow:From /home/jupyter/multi-center-fed-learning/models/model.py:42: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "2022-01-11 04:18:27.128492: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22579200 exceeds 10% of system memory.\n",
      "2022-01-11 04:18:27.128501: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22579200 exceeds 10% of system memory.\n",
      "2022-01-11 04:18:27.179104: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22579200 exceeds 10% of system memory.\n",
      "2022-01-11 04:18:27.179105: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22579200 exceeds 10% of system memory.\n",
      "2022-01-11 04:18:27.234947: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22579200 exceeds 10% of system memory.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"experiment.py\", line 92, in <module>\n",
      "    main()\n",
      "  File \"experiment.py\", line 80, in main\n",
      "    metric = trainer.begins(config, args)\n",
      "  File \"/home/jupyter/multi-center-fed-learning/models/fedrobust.py\", line 135, in begins\n",
      "    _, tmp_data = server.train_model(None, 1, batch_size, None, clients, False)\n",
      "  File \"/home/jupyter/multi-center-fed-learning/models/server.py\", line 66, in train_model\n",
      "    comp, num_samples, update = c.train(num_epochs, batch_size, minibatch, apply_prox)\n",
      "  File \"/home/jupyter/multi-center-fed-learning/models/client.py\", line 35, in train\n",
      "    comp, update = self.model.train(data, num_epochs, batch_size)\n",
      "  File \"/home/jupyter/multi-center-fed-learning/models/model.py\", line 108, in train\n",
      "    self.run_epoch(data, batch_size)\n",
      "  File \"/home/jupyter/multi-center-fed-learning/models/model.py\", line 117, in run_epoch\n",
      "    input_data = self.process_x(batched_x)\n",
      "  File \"/home/jupyter/multi-center-fed-learning/models/femnist/cnn.py\", line 77, in process_x\n",
      "    return np.array(raw_x_batch)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!/home/jupyter/dev/mlenv/bin/python experiment.py -experiment fedrobust -dataset femnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fad5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
