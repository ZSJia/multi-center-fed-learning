{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fedsem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fedsem.py\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from baseline_constants import ACCURACY_KEY\n",
    "\n",
    "\n",
    "\n",
    "import metrics.writer as metrics_writer\n",
    "STAT_METRICS_PATH = 'metrics/stat_metrics.csv'\n",
    "SYS_METRICS_PATH = 'metrics/sys_metrics.csv'\n",
    "\n",
    "\n",
    "# below import fedmc necessary lib\n",
    "from mh_constants import VARIABLE_PARAMS\n",
    "from mlhead_clus_server import Mlhead_Clus_Server\n",
    "from mlhead_utilfuncs import get_tensor_from_localmodels,count_num_point_from, log_history, save_historyfile\n",
    "\n",
    "from baseline_constants import MAIN_PARAMS, MODEL_PARAMS\n",
    "from mlhead_client import Client\n",
    "from server import Server, MDLpoisonServer\n",
    "from model import ServerModel\n",
    "from fedprox_optimizer import PerturbedGradientDescent\n",
    "from kbmom.utils import loglikelihood, BIC\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "\n",
    "def mlhead_print_totloss(k, eval_every, rounds, prefix, accuracy, cluster, stack_list, client_list):\n",
    "    print(\"acc list dimension: \", accuracy.ndim)\n",
    "#     micro_acc = np.mean(accuracy)\n",
    "    micro_acc = np.max(accuracy)\n",
    "    print('micro (with weight) test_%s: %g' % (prefix, micro_acc) )\n",
    "    #save_metric_csv(k+1, micro_acc, stack_list)\n",
    "    macro_acc = np.mean([stack_list[cl] for cl in stack_list])\n",
    "    print('macro (overall) test_%s: %g' % (prefix, macro_acc) )\n",
    "    log_history(k+1, micro_acc, macro_acc, client_list)\n",
    "    return micro_acc\n",
    "\n",
    "\n",
    "def mlhead_print_stats(\n",
    "    num_round, server, clients, num_samples, config, stack_list, prepare_test, acc_array = None):\n",
    "    \n",
    "    train_stat_metrics = server.test_model(clients, set_to_use='train')\n",
    "    print_metrics(train_stat_metrics, num_samples, prefix='train_')\n",
    "\n",
    "    test_stat_metrics = server.test_model(clients, set_to_use='test')\n",
    "    for k in test_stat_metrics:\n",
    "        stack_list[k] = test_stat_metrics[k][ACCURACY_KEY]\n",
    "  \n",
    "    test_acc =  print_metrics(test_stat_metrics, num_samples, prefix='test_')\n",
    "\n",
    "    # We also wants to evaluate a macro value (accuracy & loss)\n",
    "    c = max(config[\"num-clusters\"], 1)\n",
    "    if acc_array is not None:\n",
    "        return np.append([acc_array], [test_acc])\n",
    "    else:\n",
    "        return np.array(test_acc)\n",
    "\n",
    "def print_metrics(metrics, weights, prefix=''):\n",
    "    \"\"\"Prints weighted averages of the given metrics.\n",
    "    \"\"\"\n",
    "    ordered_weights = [weights[c] for c in sorted(weights)]\n",
    "    metric_names = metrics_writer.get_metrics_names(metrics)\n",
    "    \n",
    "    micro_acc = metric_names[0]\n",
    "    miacc_metric = [metrics[c][micro_acc] for c in sorted(metrics)]\n",
    "    to_ret = np.average(miacc_metric, weights=ordered_weights)\n",
    "    for metric in metric_names[:2]:\n",
    "        ordered_metric = [metrics[c][metric] for c in sorted(metrics)]\n",
    "        print('%s: %g, 10th percentile: %g, 50th percentile: %g, 90th percentile %g' \\\n",
    "              % (prefix + metric,\n",
    "                 np.average(ordered_metric, weights=ordered_weights),\n",
    "                 np.percentile(ordered_metric, 10),\n",
    "                 np.percentile(ordered_metric, 50),\n",
    "                 np.percentile(ordered_metric, 90)))\n",
    "\n",
    "    return to_ret\n",
    "\n",
    "class Fedsem_Trainer():\n",
    "    \n",
    "    def __init__(self, users, groups, train_data, test_data): \n",
    "        self.users = users\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.last_round = 0.\n",
    "        self._bic = 0.\n",
    "        self._db_score = 0.\n",
    "\n",
    "\n",
    "    def center_init(self, num_clusters, client_model):\n",
    "        for i in range(num_clusters):\n",
    "            if os.path.exists(\"cnn-C{}.pb\".format(i)):\n",
    "                with open(\"cnn-C{}.pb\".format(i), \"rb\") as f:\n",
    "                    self.center_models[i] = pickle.load(f)\n",
    "            else:\n",
    "                self.center_models[i] = copy.deepcopy(client_model.get_params())\n",
    "                \n",
    "    def default_cluster(self):\n",
    "        group = len(self.clients), self.clients\n",
    "        default_list = list()\n",
    "        default_list.append(group)\n",
    "        return default_list\n",
    "    \n",
    "    def clustering_function(self, points):\n",
    "        start_time = time.time()\n",
    "        # comment one clustering to use another\n",
    "        # this is outlier ones\n",
    "        iter_stop = 0\n",
    "#         learned_cluster = self.mlhead_cluster.outlier_clustering(points)\n",
    "#         while (self.mlhead_cluster.is_unbalanced_clus(learned_cluster)) and (iter_stop < 2):\n",
    "#             iter_stop += 1\n",
    "#             learned_cluster = self.mlhead_cluster.outlier_clustering(points)\n",
    "                   \n",
    "        learned_cluster = self.mlhead_cluster.run_clustering(points)     \n",
    "        end_time = time.time() - start_time\n",
    "        self.kmeans_cost.append(end_time) \n",
    "        return learned_cluster\n",
    "    \n",
    "    def evaluate(self, points):\n",
    "        data = [points[k] for k in points]\n",
    "        centers, label = self.mlhead_cluster._clusterModel.assign_clusters(data)\n",
    "        print(\"Evaluation metrics below: \")\n",
    "        \n",
    "        _sil_score = silhouette_score(data, label, metric='euclidean')\n",
    "        db_score = davies_bouldin_score(data, label)\n",
    "        print(\"BIC: \", _sil_score)\n",
    "        print(\"DB_score:\", db_score)       \n",
    "        return (_sil_score, db_score)\n",
    "        \n",
    "    \n",
    "    def model_config(self, config, dataset, my_model, seed):\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        model_path = '%s/%s.py' % (dataset, my_model)\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please specify a valid dataset and a valid model.')\n",
    "        model_path = '%s.%s' % (dataset, my_model)\n",
    "\n",
    "        print('############################## %s ##############################' % model_path)\n",
    "        mod = importlib.import_module(model_path)\n",
    "        ClientModel = getattr(mod, 'ClientModel')\n",
    "\n",
    "\n",
    "        model_params = MODEL_PARAMS[model_path]\n",
    "        if config[\"lr\"] != -1:\n",
    "            model_params_list = list(model_params)\n",
    "            model_params_list.insert(0, config[\"seed\"])\n",
    "            model_params_list[1] = config[\"lr\"]\n",
    "            model_params = tuple(model_params_list)\n",
    "\n",
    "        # Create client model, and share params with server model\n",
    "        tf.reset_default_graph()\n",
    "        client_model = ClientModel(*model_params)\n",
    "        num_clusters = config[\"num-clusters\"]\n",
    "        \n",
    "\n",
    "        \n",
    "        # Create clients\n",
    "        _users = self.users\n",
    "        groups = [[] for _ in _users]\n",
    "        clients =  [Client(u, g, self.train_data[u], self.test_data[u], client_model) \\\n",
    "                    for u, g in zip(_users, groups)]\n",
    "        print('%d Clients in Total' % len(clients)) \n",
    "        \n",
    "        if config['poisoning'] == True:\n",
    "            num_agents = int(config[\"num_agents\"] * len(clients)) \n",
    "            clients_per_round = config[\"clients-per-round\"]\n",
    "            server = MDLpoisonServer(client_model, clients, num_agents, model_params[2], clients_per_round)\n",
    "        else:\n",
    "            # Create server\n",
    "            server = Server(client_model)            \n",
    "\n",
    "        client_ids, client_groups, all_num_samples = server.get_clients_info(clients)\n",
    "        # Create our SEM modeling agl\n",
    "        print('--- Random Initialization ---')\n",
    "\n",
    "        print(\"--- Do training and initilized clusting server---\")\n",
    "        self.mlhead_cluster = Mlhead_Clus_Server(client_model, dataset, \"cnn\", num_clusters, len(clients), seed)\n",
    "        self.mlhead_cluster.select_clients(seed,  clients)\n",
    "        self.center_models = [None] * num_clusters\n",
    "        self.center_init(num_clusters, client_model)\n",
    "        \n",
    "        return clients, server, client_model, all_num_samples\n",
    "\n",
    "    def begins(self, config, args):\n",
    "        clients, server, client_model, all_num_samples = self.model_config(config, args.dataset, 'cnn', config[\"seed\"])         \n",
    "        \"\"\"\n",
    "            A trainer different from the baseline\n",
    "            Then all need to do is replace different optimizer\n",
    "        \"\"\"\n",
    "        num_clusters = config[\"num-clusters\"]\n",
    "        num_rounds = config[\"num-rounds\"]\n",
    "        eval_every = config[\"eval-every\"]\n",
    "        epochs_per_round = config['epochs']\n",
    "        batch_size = config['batch-size']\n",
    "        clients_per_round = config[\"clients-per-round\"]\n",
    "        update_head_every = config['update-center-every']\n",
    "        \n",
    "        print(\"----- Multi-center Federated Training -----\")\n",
    "        prev_score = None\n",
    "        self.kmeans_cost = []\n",
    "        for k in range(num_rounds):\n",
    "            best_kept = None\n",
    "            stack_list = {}\n",
    "            client_list = {}\n",
    "            if prev_score is None: # This is the first iteration\n",
    "                if num_clusters == -1 :\n",
    "                    write_file = False\n",
    "                    learned_cluster = self.default_cluster()\n",
    "                else:\n",
    "                    print(\"----- First time center rendering  -----\")\n",
    "                    write_file = True\n",
    "                               \n",
    "                    c_wts = self.mlhead_cluster.get_init_point_data()\n",
    "                    learned_cluster = self.clustering_function(c_wts)\n",
    "                    prev_score = len(c_wts)\n",
    "\n",
    "            #print('--- Round %d of %d: Training %d Clients ---' % (k + 1, num_rounds, clients_per_round))\n",
    "            print('--- Round %d of %d: Training assigned to %d Cluster ' % (k + 1, num_rounds, \n",
    "                                                                            len(learned_cluster)), \n",
    "                \"<\", count_num_point_from(learned_cluster), \"> ---\")\n",
    "\n",
    "            joined_clients = dict()\n",
    "            for c_idx, group in enumerate(learned_cluster):\n",
    "                server = Server(None)\n",
    "                if group[0] <= 1:\n",
    "                    print(\"Skip cluster %d as number of client not enough\" % c_idx)\n",
    "                    continue\n",
    "                # if not explicit clients per round given, then default all\n",
    "                # clients in this group participarte training\n",
    "                if clients_per_round != -1: \n",
    "                    num = min(clients_per_round, group[0])\n",
    "                    active_clients = np.random.choice(group[1], num, replace=False)\n",
    "                else:\n",
    "                    active_clients = group[1]\n",
    "                \n",
    "                client_list[c_idx] = [c.id for c in active_clients ]\n",
    "                c_ids, c_groups, c_num_samples = server.get_clients_info(active_clients)\n",
    "                sys_metrics, updates = server.train_model(self.center_models[c_idx], \n",
    "                                                          num_epochs=epochs_per_round, \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          clients = active_clients)\n",
    "                \n",
    "                for c, up in zip(active_clients, updates):\n",
    "                    joined_clients[c.id] = up[1]\n",
    "                # Thinking how to do a distance as weight averging\n",
    "                if  (\"mode\" not in config) or (config[\"mode\"])  == \"no_size\":\n",
    "                    self.center_models[c_idx] = server.update_model_wmode()\n",
    "                else:\n",
    "                    server.update_model_nowmode()\n",
    "\n",
    "                #_, _, client_num_samples = server.get_clients_info(active_clients)\n",
    "                best_kept = mlhead_print_stats(k + 1, server, clients, all_num_samples, config, \n",
    "                                                   stack_list, True, best_kept)\n",
    "            # end iterate clusters\n",
    "            self.last_round = mlhead_print_totloss(\n",
    "                k, eval_every, num_rounds, \"accuracy\",  best_kept, learned_cluster, stack_list, client_list)\n",
    "\n",
    "            # Update the center point when k = local training * a mulitplier\n",
    "            if not num_clusters == -1 and not k == (num_rounds -1) and (k + 1) % update_head_every == 0:\n",
    "                c_wts = get_tensor_from_localmodels(joined_clients,\n",
    "                                                  c_wts,\n",
    "                                                  self.mlhead_cluster.variable, client_model)\n",
    "                learned_cluster = self.clustering_function(c_wts) # cwts is N (clients) x x_dimensions\n",
    "                joined_clients.clear()\n",
    "                print(\"----- center update performed -----\")\n",
    "                prev_score = len(c_wts)\n",
    "                \n",
    "        client_model.close()\n",
    "        \n",
    "        if config[\"benchmark\"] == 1:\n",
    "            vals = self.evaluate(c_wts)\n",
    "            self._bic = vals[0]\n",
    "            self._db_score = vals[1]        \n",
    "        return self.last_round\n",
    "                \n",
    "\n",
    "    def ends(self):\n",
    "        save_historyfile()\n",
    "        print(\"experiment of Fedsem finished.\")\n",
    "        return\n",
    "        # save history file\n",
    "        \n",
    "        # Save server model        \n",
    "#         ckpt_path = os.path.join('checkpoints', args.dataset)\n",
    "#         if not os.path.exists(ckpt_path):\n",
    "#             os.makedirs(ckpt_path)\n",
    "\n",
    "#         for i, server in enumerate(self.head_server_stack):\n",
    "#             # {}-K{}-C{}, K stands for number of clusters and C stands for ith center\n",
    "#             save_path = server.save_model(os.path.join(ckpt_path, '{}-K{}-C{}.ckpt'.format(args.model, args.num_clusters, i+1)))\n",
    "#         print('Model saved in path: %s' % save_path)\n",
    "#        print('{} rounds kmeans used {:.3f}'.format(self.num_rounds, np.average(self.kmeans_cost, weights=None)))\n",
    "#         for i, server in enumerate(self.center_models):\n",
    "#             head_weights = server\n",
    "#             with open('./{}-C{}.pb'.format(args.model, i), 'wb+') as f:\n",
    "#                 pickle.dump(head_weights, f)\n",
    "                \n",
    "#         for s in self.head_server_stack:\n",
    "#             s.close_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
