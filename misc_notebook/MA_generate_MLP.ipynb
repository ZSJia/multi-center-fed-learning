{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I8NeuQl9cZEt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niD3PLsPcmCm",
    "outputId": "3728bd1a-aecf-4b26-c235-8b04ea06ffdf"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "num_workers = 20\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "\n",
    "idxs = np.random.permutation(n_train)\n",
    "batch_idxs = np.array_split(idxs, num_workers)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SieNEHEOco7N"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "def create_mlp():\n",
    "  model = models.Sequential()\n",
    "  model.add(tf.keras.Input(shape=(28,28)))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(100, activation='relu'))\n",
    "  model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "      metrics=['accuracy']\n",
    "  ) \n",
    "\n",
    "  return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkEL3AC_d7HQ",
    "outputId": "c73d75a1-5dc3-4608-956b-bc7dbd5d3b26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tf1gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_mlp()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HOXEl6hd-JW",
    "outputId": "5ce60332-35a4-4ca3-ac6e-28f4ce9f2d83"
   },
   "outputs": [],
   "source": [
    "for j in range(num_workers):\n",
    "  model = create_mlp()\n",
    "  train_features, train_labels = x_train[batch_idxs[j]], y_train[batch_idxs[j]]\n",
    "  model.fit(train_features, train_labels, batch_size=32, epochs=9, validation_data=(x_test, y_test))\n",
    "\n",
    "  Ws = model.get_weights()\n",
    "  with open(\"mni_workernn_{}\".format(j), 'wb+') as f:\n",
    "    pickle.dump(Ws, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 77us/sample - loss: 0.4862 - acc: 0.8803\n",
      "[0.4862384514331818, 0.8803]\n",
      "10000/10000 [==============================] - 1s 75us/sample - loss: 0.5022 - acc: 0.8758\n",
      "[0.5021830556154251, 0.8758]\n",
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.4963 - acc: 0.8715\n",
      "[0.4963450841426849, 0.8715]\n",
      "10000/10000 [==============================] - 1s 78us/sample - loss: 0.4987 - acc: 0.8705\n",
      "[0.4986572093963623, 0.8705]\n",
      "10000/10000 [==============================] - 1s 77us/sample - loss: 0.4907 - acc: 0.8736\n",
      "[0.49069374022483825, 0.8736]\n",
      "10000/10000 [==============================] - 1s 78us/sample - loss: 0.4976 - acc: 0.8769\n",
      "[0.4976419484376907, 0.8769]\n",
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.4880 - acc: 0.8817\n",
      "[0.48803191804885865, 0.8817]\n",
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.5079 - acc: 0.8686\n",
      "[0.5078792052268982, 0.8686]\n",
      "10000/10000 [==============================] - 1s 78us/sample - loss: 0.4793 - acc: 0.8778\n",
      "[0.47933087840080263, 0.8778]\n",
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.4905 - acc: 0.8759\n",
      "[0.4905010535001755, 0.8759]\n",
      "10000/10000 [==============================] - 1s 80us/sample - loss: 0.4861 - acc: 0.8768\n",
      "[0.48608291573524476, 0.8768]\n",
      "10000/10000 [==============================] - 1s 80us/sample - loss: 0.5011 - acc: 0.8701\n",
      "[0.501139972114563, 0.8701]\n",
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.4832 - acc: 0.8740\n",
      "[0.48320589809417724, 0.874]\n",
      "10000/10000 [==============================] - 1s 92us/sample - loss: 0.4854 - acc: 0.8739\n",
      "[0.4853775868654251, 0.8739]\n",
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.4995 - acc: 0.8690\n",
      "[0.49945286605358125, 0.869]\n",
      "10000/10000 [==============================] - 1s 84us/sample - loss: 0.4944 - acc: 0.8775\n",
      "[0.4944356895685196, 0.8775]\n",
      "10000/10000 [==============================] - 1s 77us/sample - loss: 0.4908 - acc: 0.8750\n",
      "[0.49078040227890013, 0.875]\n",
      "10000/10000 [==============================] - 1s 80us/sample - loss: 0.4833 - acc: 0.8774\n",
      "[0.48326182775497434, 0.8774]\n",
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.4935 - acc: 0.8725\n",
      "[0.4935495052576065, 0.8725]\n",
      "10000/10000 [==============================] - 1s 82us/sample - loss: 0.4880 - acc: 0.8764\n",
      "[0.487951301074028, 0.8764]\n"
     ]
    }
   ],
   "source": [
    "for j in range(num_workers):\n",
    "    model = create_mlp()\n",
    "    with open(\"mni_workernn_{}\".format(j), 'rb') as f:\n",
    "        ws = pickle.load(f)\n",
    "    model.set_weights(ws)\n",
    "    print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for cifa10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "def create_mlp():\n",
    "    model = models.Sequential()\n",
    "    # model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    # model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    # model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1536, activation='relu'))\n",
    "    model.add(layers.Dense(384, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(0.001, momentum=0.9, nesterov=True),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 5s 99us/sample - loss: 1.8856 - acc: 0.3257 - val_loss: 1.7566 - val_acc: 0.3685\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 5s 97us/sample - loss: 1.6746 - acc: 0.4098 - val_loss: 1.6280 - val_acc: 0.4235\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 5s 97us/sample - loss: 1.5809 - acc: 0.4400 - val_loss: 1.5870 - val_acc: 0.4316\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 5s 97us/sample - loss: 1.5151 - acc: 0.4617 - val_loss: 1.5294 - val_acc: 0.4493\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 5s 98us/sample - loss: 1.4621 - acc: 0.4831 - val_loss: 1.4948 - val_acc: 0.4657\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 5s 98us/sample - loss: 1.4128 - acc: 0.4995 - val_loss: 1.5732 - val_acc: 0.4348\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 5s 97us/sample - loss: 1.3770 - acc: 0.5121 - val_loss: 1.4659 - val_acc: 0.4679\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 5s 97us/sample - loss: 1.3386 - acc: 0.5254 - val_loss: 1.4065 - val_acc: 0.4970\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 5s 98us/sample - loss: 1.3024 - acc: 0.5387 - val_loss: 1.3720 - val_acc: 0.5098\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 5s 98us/sample - loss: 1.2760 - acc: 0.5480 - val_loss: 1.3745 - val_acc: 0.5072\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 94us/sample - loss: 1.3745 - acc: 0.5072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3744550441741943, 0.5072]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tf1gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "num_workers = 40\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "\n",
    "idxs = np.random.permutation(n_train)\n",
    "batch_idxs = np.array_split(idxs, num_workers)   \n",
    "\n",
    "for j in range(num_workers):\n",
    "  model = create_mlp()\n",
    "  train_features, train_labels = x_train[batch_idxs[j]], y_train[batch_idxs[j]]\n",
    "  model.fit(train_features, train_labels, batch_size=64, epochs=10, validation_data=(x_test, y_test), verbose=0)\n",
    "\n",
    "  Ws = model.get_weights()\n",
    "  with open(\"cifar10_{}.pb\".format(j), 'wb+') as f:\n",
    "    pickle.dump(Ws, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = {i: dataidx for i, dataidx in enumerate(batch_idxs)}\n",
    "\n",
    "def saved_cls_counts(net_dataidx_map):\n",
    "    net_cls_counts = {}\n",
    "\n",
    "    for net_i, dataidx in net_dataidx_map.items():\n",
    "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "        net_cls_counts[net_i] = tmp\n",
    "        \n",
    "    with open('cifar10_counts', 'wb+') as f:\n",
    "        pickle.dump(net_cls_counts, f)\n",
    "\n",
    "saved_cls_counts(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([ 4219, 42286,  6642, ..., 48874, 48800, 48710]), 1: array([15990, 14719,  7778, ..., 10015, 34386, 45670]), 2: array([32013, 19734, 25504, ..., 41383,  7567, 10785]), 3: array([23218, 46749, 36137, ..., 12351, 41417, 33904]), 4: array([46143, 13257, 12382, ...,  2293, 18944, 45370]), 5: array([30980, 10077, 41684, ..., 22458, 36590, 41647]), 6: array([   75, 11978, 39749, ..., 19506, 14970, 41265]), 7: array([42988, 38910,  1386, ..., 26642, 12346,  3922]), 8: array([24392, 32694, 13181, ..., 14172,    91, 22183]), 9: array([33608, 49368, 46530, ..., 22613, 20269, 48593]), 10: array([49001, 25241, 41362, ...,  5497, 37134,   412]), 11: array([22660, 23078, 17909, ...,  8835, 34090, 34423]), 12: array([11248, 48604, 17840, ..., 32441, 23911, 42153]), 13: array([22284, 29144,  7871, ..., 28035, 39055, 45679]), 14: array([11273, 25669, 22939, ..., 12365, 33458, 26230]), 15: array([15090, 19619, 39679, ...,  9264, 47016, 23121]), 16: array([32600,  5187, 15626, ..., 32022, 39481, 40638]), 17: array([46102,  1198,  3002, ..., 36444, 10665,  2013]), 18: array([10000, 13331, 42150, ...,  8840, 26953, 22394]), 19: array([10318, 33836, 36583, ...,  4815, 12484, 34572]), 20: array([38419, 45255, 38640, ..., 43995, 25055, 44657]), 21: array([27721, 19797, 42950, ..., 11984, 30144,  9885]), 22: array([46402, 30366,    71, ..., 30292, 22360, 39202]), 23: array([26467, 32650, 12094, ..., 44771,  2488, 31431]), 24: array([30702,  3549, 44603, ..., 49765, 29963, 35127]), 25: array([32595, 39989, 19923, ..., 37867, 10219, 21256]), 26: array([23788, 44147, 29019, ..., 40181, 21341, 25846]), 27: array([32771, 35894, 24839, ..., 39198, 28582, 49599]), 28: array([36840, 12267,  2897, ..., 42871, 15362, 32140]), 29: array([ 4096, 36484, 10143, ..., 32316, 48132, 34895]), 30: array([48639, 34957, 19388, ..., 42068, 18768, 11610]), 31: array([47644, 20585, 24700, ..., 31762, 33030, 39526]), 32: array([49885, 49497, 28865, ..., 11340, 43023, 14541]), 33: array([23951, 46564, 11783, ..., 34589,  5095, 25870]), 34: array([17254,  2706, 37729, ..., 15908, 12838, 10101]), 35: array([23601, 34989, 33389, ...,  3819, 36345, 35743]), 36: array([ 5972, 49506, 11705, ..., 34294,  9005,  1888]), 37: array([15053, 24311,  3217, ...,  7887, 25584,  1513]), 38: array([  450, 35827,  5871, ..., 23795, 31289, 27355]), 39: array([25754, 12709, 23407, ..., 46133, 43085, 11282])}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MA-generate-MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
